{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from autogen import UserProxyAgent, AssistantAgent\n",
    "from twikit import Client\n",
    "import os\n",
    "\n",
    "keyword = \"Artificial Intelligence\"\n",
    "topic_count = 20\n",
    "article_count = 10\n",
    "max_period_hours = 3\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "GROQ_MODEL_NAME = os.getenv(\"GROQ_MODEL_NAME\")\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "GROQ_API_BASE = os.getenv(\"GROQ_API_BASE\")\n",
    "RELEASE = os.getenv(\"RELEASE\")\n",
    "\n",
    "if(RELEASE == \"PROD\"):\n",
    "    USERNAME = os.getenv(\"XUSERNAME\")\n",
    "    EMAIL = os.getenv(\"XEMAIL\")\n",
    "    PASSWORD = os.getenv(\"XPASSWORD\")\n",
    "else:\n",
    "    USERNAME = os.getenv(\"XUSERNAME_TEST\")\n",
    "    EMAIL = os.getenv(\"XEMAIL_TEST\")\n",
    "    PASSWORD = os.getenv(\"XPASSWORD_TEST\")\n",
    "\n",
    "# Config dictionary\n",
    "llm_config = {\n",
    "    \"cache_seed\": 42,\n",
    "    \"config_list\": [{\n",
    "        \"model\": GROQ_MODEL_NAME,\n",
    "        \"api_key\": GROQ_API_KEY,\n",
    "        \"base_url\": GROQ_API_BASE\n",
    "    }],\n",
    "}\n",
    "\n",
    "# # Initialize client\n",
    "if 'x_client' not in globals():\n",
    "    x_client = Client('en-US')\n",
    "\n",
    "    x_client.login(\n",
    "        auth_info_1=USERNAME,\n",
    "        auth_info_2=EMAIL,\n",
    "        password=PASSWORD\n",
    "    )\n",
    "    print(\"Client initialized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from newspaper import Article\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def select_random_article(news_list):\n",
    "    if not os.path.isfile('urls.csv'):\n",
    "        df_urls = pd.DataFrame(columns=['urls', 'status'])  # Define the variable with a default value\n",
    "        df_urls.to_csv('urls.csv')\n",
    "    else:\n",
    "        df_urls = pd.read_csv('urls.csv', index_col='Unnamed: 0')\n",
    "    news, article = None, None\n",
    "    \n",
    "    while True:\n",
    "        #remove any news from the news_list if it is already in the csv file\n",
    "        news_list = [news for news in news_list if news['url'] not in df_urls['urls'].values]\n",
    "        print(f'FILTERED NEWS LIST: {news_list}')\n",
    "        if not news_list:\n",
    "            print(\"No more news to select\")\n",
    "            return None, None\n",
    "        \n",
    "        news = random.choice(news_list)\n",
    "        try:\n",
    "            # Code to check if the article is valid\n",
    "            # For example, you can check if the URL is accessible or if the content is not empty\n",
    "            # If the article is valid, return it\n",
    "            # Otherwise, continue to the next iteration of the loop\n",
    "            article = Article(news['url'])\n",
    "            article.download()\n",
    "            article.parse()\n",
    "\n",
    "            if article.text and len(article.text.strip().split('\\n')) > 1:\n",
    "                # Append the URL and status to the DataFrame\n",
    "                df_urls = pd.concat([pd.DataFrame([[news['url'], 'success']], columns=df_urls.columns), df_urls], ignore_index=True)\n",
    "                df_urls.to_csv('urls.csv')\n",
    "                break\n",
    "            else:\n",
    "                df_urls = pd.concat([pd.DataFrame([[news['url'], 'empty']], columns=df_urls.columns), df_urls], ignore_index=True)\n",
    "                df_urls.to_csv('urls.csv')\n",
    "                continue\n",
    "        except Exception as e:\n",
    "            # Append the URL and status to the DataFrame\n",
    "            df_urls = pd.concat([pd.DataFrame([[news['url'], 'error']], columns=df_urls.columns), df_urls], ignore_index=True)\n",
    "            df_urls.to_csv('urls.csv')\n",
    "            print(f\"Error selecting article: {str(e)}\")\n",
    "            continue\n",
    "    return news, article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from gnews import GNews\n",
    "from pyshorteners import Shortener\n",
    "\n",
    "#delete .cache/topics.csv to reset the topics\n",
    "if os.path.isfile('.cache/topics.csv'):\n",
    "    os.remove('.cache/topics.csv')\n",
    "\n",
    "def topic_selection_tool(topics_list: Annotated[list, \"The list of topics\"] = None) -> str:\n",
    "    #set df_topics topic column from the topics_list and set status to pending if it's not already in the df_topics\n",
    "    if os.path.isfile('.cache/topics.csv'):\n",
    "        df_topics = pd.read_csv('.cache/topics.csv', index_col='Unnamed: 0')\n",
    "    else:\n",
    "        df_topics = pd.DataFrame(columns=['topic', 'status'])  # Define the variable with a default value\n",
    "    if topics_list:\n",
    "        for topic in topics_list:\n",
    "            if topic not in df_topics['topic'].values:\n",
    "                df_topics = pd.concat([df_topics, pd.DataFrame([[topic, 'pending']], columns=df_topics.columns)], ignore_index=True)\n",
    "    \n",
    "    topics_list = df_topics[df_topics['status'] == 'pending']['topic'].values.tolist()\n",
    "    if(len(topics_list) == 0):\n",
    "        return \"No more topics to select\"\n",
    "    \n",
    "    topic_selected = random.choice(topics_list)\n",
    "    df_topics.loc[df_topics['topic'] == topic_selected, 'status'] = 'selected'\n",
    "    \n",
    "    df_topics.to_csv('.cache/topics.csv')\n",
    "    return topic_selected\n",
    "\n",
    "def get_news_article_tool(topic: Annotated[str, \"The topic to collect news on\"], count: Annotated[int, \"The number of news articles to collect from the internet\"]) -> str:\n",
    "    google_news = GNews()\n",
    "    google_news.max_results = count  # number of responses across a keyword\n",
    "    google_news.language = 'english'  # News in a specific language\n",
    "    period_hours = 1\n",
    "    \n",
    "    while True:\n",
    "        google_news.period = f'{period_hours}h'  # Adjust period in hours\n",
    "        news_list = google_news.get_news(topic)\n",
    "        news, article = select_random_article(news_list)\n",
    "        print(f'NEWS LIST: {news_list}')\n",
    "\n",
    "        if news and article:\n",
    "            s = Shortener(timeout=5)\n",
    "            short_url = s.tinyurl.short(news['url'])\n",
    "\n",
    "            result = (\n",
    "    \"\"\"TITLE: {title}\n",
    "\n",
    "CONTENT: {content}\n",
    "\n",
    "SOURCE: {url}\"\"\"\n",
    "            ).format(\n",
    "                title=news['title'],\n",
    "                content=article.text.replace('\\n\\n', '\\n'),\n",
    "                url=short_url\n",
    "            )\n",
    "            return result\n",
    "        if period_hours >= max_period_hours:\n",
    "            topic = topic_selection_tool(None)\n",
    "            period_hours = 0\n",
    "        period_hours += 1  # Increase the period by 1 hour and try again\n",
    "    \n",
    "# def get_trending_tweets_tool(topic: Annotated[str, \"The topic to retrieve tweets on\"], count: Annotated[int, \"The number of top tweets to collect\"]) -> str:\n",
    "#     tweets = []\n",
    "#     tweets_batch = x_client.search_tweet(query=topic, product='Top', count=count)\n",
    "\n",
    "#     while len(tweets) < count:\n",
    "#         for tweet in tweets_batch:\n",
    "#             if tweet.lang == 'en' and 't.co/' in tweet.full_text:\n",
    "#                 tweets.append(tweet.full_text)\n",
    "#                 if len(tweets) >= count:\n",
    "#                     break\n",
    "#         else:\n",
    "#             tweets_batch = tweets_batch.next()\n",
    "\n",
    "#     return \"\\n\\n\".join(f'Tweet {i+1}: \"{tweet}\"' for i, tweet in enumerate(tweets))\n",
    "\n",
    "def write_tweet_tool(tweet: Annotated[str, \"The tweet to post\"], source: Annotated[str, \"The source URL of the news\"]) -> str:\n",
    "    try:\n",
    "        tweet = \"New research from Cohere for AI presents a comprehensive study on multilingual preference optimization in NLP. Training effective multilingual language models requires high-quality, diverse, multilingual data. This study sets a new benchmark and highlights the value of online training methods. #AI #ML #NLP #Multilingual\"\n",
    "        source = \"tinyurl.com/2aqxoal9\"\n",
    "\n",
    "        if 'tinyurl.com' in tweet:\n",
    "            if len(tweet) > 280:\n",
    "                tweet = tweet[:276-len(source)] + '...' + f\"\\n{source}\"\n",
    "        else:\n",
    "            if len(tweet) <= 279-len(source):\n",
    "                tweet += f\"\\n{source}\"\n",
    "            else:\n",
    "                tweet = tweet[:276-len(source)] + '...' + f\"\\n{source}\"\n",
    "        \n",
    "        if RELEASE != \"DEV\":\n",
    "            x_client.create_tweet(\n",
    "                text=tweet,\n",
    "            )\n",
    "        \n",
    "        return f'Tweet posted: \"{tweet}\"'\n",
    "    except Exception as e:\n",
    "        error_message = f\"Failed to post tweet: {str(e)}\"\n",
    "        return error_message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_selector_agent = AssistantAgent(\n",
    "    \"topic_selector_agent\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=f\"You are good at writing a list of closely related topics to the given topic(including the given topic) and then choose any one out of them. Use the provided tools for both topic selection and to collect new articles.\",\n",
    "    max_consecutive_auto_reply=1\n",
    ")\n",
    "\n",
    "news_collector_agent = AssistantAgent(\n",
    "    \"news_collector_agent\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=f\"You are good at collecting recent news articles about a given topic on the internet. Use the provided tool to collect news about the chosen topic.\",\n",
    "    max_consecutive_auto_reply=1\n",
    ")\n",
    "\n",
    "# news_picker_agent = AssistantAgent(\n",
    "#     \"news_picker_agent\",\n",
    "#     llm_config=llm_config,\n",
    "#     system_message=\"You are good at picking the most interesting news article from a list of given news articles AS IT IS. Always include the source URL link\",\n",
    "#     max_consecutive_auto_reply=1\n",
    "# )\n",
    "\n",
    "# tweets_retriever_agent = AssistantAgent(\n",
    "#     \"tweets_retriever_agent\",\n",
    "#     llm_config=llm_config,\n",
    "#     system_message=\"You are good at retrieving recent tweets about a given topic on twitter. Always include source the URL link. Use the provided tool.\",\n",
    "#     max_consecutive_auto_reply=1\n",
    "# )\n",
    "\n",
    "tweet_writer_agent = AssistantAgent(\n",
    "    \"tweet_writer_agent\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=\"You are an autonomous twitter bot that's designed to post the latest news for everyone. You are good at posting twitter posts on the given news. Always use simple words. Use the provided tool to post a tweet.\",\n",
    "    max_consecutive_auto_reply=1\n",
    ")\n",
    "\n",
    "\n",
    "user_proxy_agent = UserProxyAgent(\n",
    "    name=\"User\",\n",
    "    system_message=\"You are a helpful AI assistant. Return 'TERMINATE' when the task is done.\",\n",
    "    is_termination_msg=lambda msg: msg.get(\"content\") is not None and \"TERMINATE\" in msg[\"content\"],\n",
    "    human_input_mode=\"NEVER\",\n",
    "    code_execution_config=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.write_tweet_tool(tweet: Annotated[str, 'The tweet to post'], source: Annotated[str, 'The source URL of the news']) -> str>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Register the tool signature with the assistant agent.\n",
    "topic_selector_agent.register_for_llm(name=\"topic_selection_tool\", description=\"Generate a list of topics related to the input topic and return a random topic.\")(topic_selection_tool)\n",
    "news_collector_agent.register_for_llm(name=\"get_news_article_tool\", description=\"Collect news articles about a topic on the internet.\")(get_news_article_tool)\n",
    "# tweets_retriever_agent.register_for_llm(name=\"get_trending_tweets_tool\", description=\"Collect top trending tweets about a topic on twitter.\")(get_trending_tweets_tool)\n",
    "tweet_writer_agent.register_for_llm(name=\"write_tweet_tool\", description=\"Write a twitter post.\")(write_tweet_tool)\n",
    "\n",
    "# Register the tool function with the user proxy agent.\n",
    "user_proxy_agent.register_for_execution(name=\"topic_selection_tool\")(topic_selection_tool)\n",
    "user_proxy_agent.register_for_execution(name=\"get_news_article_tool\")(get_news_article_tool)\n",
    "# user_proxy_agent.register_for_execution(name=\"get_trending_tweets_tool\")(get_trending_tweets_tool)\n",
    "user_proxy_agent.register_for_execution(name=\"write_tweet_tool\")(write_tweet_tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[34mStarting a new chat....\u001b[0m\n",
      "\u001b[34m\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[33mUser\u001b[0m (to topic_selector_agent):\n",
      "\n",
      "Generate a list of 20 topics related to the topic 'Artificial Intelligence' and return a random topic from the list.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mtopic_selector_agent\u001b[0m (to User):\n",
      "\n",
      "\u001b[32m***** Suggested tool call (call_rmw8): topic_selection_tool *****\u001b[0m\n",
      "Arguments: \n",
      "{\"topics_list\":[\"Artificial Intelligence\",\"Machine Learning\",\"Deep Learning\",\"Neural Networks\",\"Natural Language Processing\",\"Computer Vision\",\"Robotics\",\"Expert Systems\",\"Speech Recognition\",\"Data Mining\",\"Predictive Analytics\",\"Reinforcement Learning\",\"Fuzzy Logic\",\"Genetic Algorithms\",\"Artificial Neural Networks\",\"Neural-Symbolic Learning Systems\",\"Cognitive Computing\",\"Affective Computing\",\"Autonomous Agents\",\"Swarm Intelligence\"]}\n",
      "\u001b[32m*****************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION topic_selection_tool...\u001b[0m\n",
      "\u001b[33mUser\u001b[0m (to topic_selector_agent):\n",
      "\n",
      "\u001b[33mUser\u001b[0m (to topic_selector_agent):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_rmw8) *****\u001b[0m\n",
      "Neural-Symbolic Learning Systems\n",
      "\u001b[32m**************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[34m\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[34mStarting a new chat....\u001b[0m\n",
      "\u001b[34m\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[33mUser\u001b[0m (to news_collector_agent):\n",
      "\n",
      "Collect 10 news articles about the given topic from the internet.\n",
      "Context: \n",
      "Neural-Symbolic Learning Systems\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/08/2024 01:22:13 PM - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mnews_collector_agent\u001b[0m (to User):\n",
      "\n",
      "\u001b[32m***** Suggested tool call (call_njke): get_news_article_tool *****\u001b[0m\n",
      "Arguments: \n",
      "{\"count\":10,\"topic\":\"Neural-Symbolic Learning Systems\"}\n",
      "\u001b[32m******************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION get_news_article_tool...\u001b[0m\n",
      "FILTERED NEWS LIST: []\n",
      "No more news to select\n",
      "NEWS LIST: []\n",
      "FILTERED NEWS LIST: []\n",
      "No more news to select\n",
      "NEWS LIST: []\n",
      "FILTERED NEWS LIST: []\n",
      "No more news to select\n",
      "NEWS LIST: []\n",
      "FILTERED NEWS LIST: []\n",
      "No more news to select\n",
      "NEWS LIST: [{'title': 'This AI Paper from Cohere for AI Presents a Comprehensive Study on Multilingual Preference Optimization - MarkTechPost', 'description': 'This AI Paper from Cohere for AI Presents a Comprehensive Study on Multilingual Preference Optimization  MarkTechPost', 'published date': 'Mon, 08 Jul 2024 16:39:31 GMT', 'url': 'https://news.google.com/rss/articles/CBMikAFodHRwczovL3d3dy5tYXJrdGVjaHBvc3QuY29tLzIwMjQvMDcvMDgvdGhpcy1haS1wYXBlci1mcm9tLWNvaGVyZS1mb3ItYWktcHJlc2VudHMtYS1jb21wcmVoZW5zaXZlLXN0dWR5LW9uLW11bHRpbGluZ3VhbC1wcmVmZXJlbmNlLW9wdGltaXphdGlvbi_SAZQBaHR0cHM6Ly93d3cubWFya3RlY2hwb3N0LmNvbS8yMDI0LzA3LzA4L3RoaXMtYWktcGFwZXItZnJvbS1jb2hlcmUtZm9yLWFpLXByZXNlbnRzLWEtY29tcHJlaGVuc2l2ZS1zdHVkeS1vbi1tdWx0aWxpbmd1YWwtcHJlZmVyZW5jZS1vcHRpbWl6YXRpb24vP2FtcA?oc=5&hl=en-US&gl=US&ceid=US:en', 'publisher': {'href': 'https://www.marktechpost.com', 'title': 'MarkTechPost'}}]\n",
      "FILTERED NEWS LIST: [{'title': \"Worrying About Sycophantism: Why I Again Tweaked the Custom GPT 'Panel of AI Experts for Lawyers' to Add More ... - JD Supra\", 'description': \"Worrying About Sycophantism: Why I Again Tweaked the Custom GPT 'Panel of AI Experts for Lawyers' to Add More ...  JD Supra\", 'published date': 'Mon, 08 Jul 2024 15:58:46 GMT', 'url': 'https://news.google.com/rss/articles/CBMiUmh0dHBzOi8vd3d3Lmpkc3VwcmEuY29tL2xlZ2FsbmV3cy93b3JyeWluZy1hYm91dC1zeWNvcGhhbnRpc20td2h5LWktYWdhaW4tMjE3MDE0Mi_SAQA?oc=5&hl=en-US&gl=US&ceid=US:en', 'publisher': {'href': 'https://www.jdsupra.com', 'title': 'JD Supra'}}]\n",
      "NEWS LIST: [{'title': \"Worrying About Sycophantism: Why I Again Tweaked the Custom GPT 'Panel of AI Experts for Lawyers' to Add More ... - JD Supra\", 'description': \"Worrying About Sycophantism: Why I Again Tweaked the Custom GPT 'Panel of AI Experts for Lawyers' to Add More ...  JD Supra\", 'published date': 'Mon, 08 Jul 2024 15:58:46 GMT', 'url': 'https://news.google.com/rss/articles/CBMiUmh0dHBzOi8vd3d3Lmpkc3VwcmEuY29tL2xlZ2FsbmV3cy93b3JyeWluZy1hYm91dC1zeWNvcGhhbnRpc20td2h5LWktYWdhaW4tMjE3MDE0Mi_SAQA?oc=5&hl=en-US&gl=US&ceid=US:en', 'publisher': {'href': 'https://www.jdsupra.com', 'title': 'JD Supra'}}, {'title': 'This AI Paper from Cohere for AI Presents a Comprehensive Study on Multilingual Preference Optimization - MarkTechPost', 'description': 'This AI Paper from Cohere for AI Presents a Comprehensive Study on Multilingual Preference Optimization  MarkTechPost', 'published date': 'Mon, 08 Jul 2024 16:39:31 GMT', 'url': 'https://news.google.com/rss/articles/CBMikAFodHRwczovL3d3dy5tYXJrdGVjaHBvc3QuY29tLzIwMjQvMDcvMDgvdGhpcy1haS1wYXBlci1mcm9tLWNvaGVyZS1mb3ItYWktcHJlc2VudHMtYS1jb21wcmVoZW5zaXZlLXN0dWR5LW9uLW11bHRpbGluZ3VhbC1wcmVmZXJlbmNlLW9wdGltaXphdGlvbi_SAZQBaHR0cHM6Ly93d3cubWFya3RlY2hwb3N0LmNvbS8yMDI0LzA3LzA4L3RoaXMtYWktcGFwZXItZnJvbS1jb2hlcmUtZm9yLWFpLXByZXNlbnRzLWEtY29tcHJlaGVuc2l2ZS1zdHVkeS1vbi1tdWx0aWxpbmd1YWwtcHJlZmVyZW5jZS1vcHRpbWl6YXRpb24vP2FtcA?oc=5&hl=en-US&gl=US&ceid=US:en', 'publisher': {'href': 'https://www.marktechpost.com', 'title': 'MarkTechPost'}}]\n",
      "\u001b[33mUser\u001b[0m (to news_collector_agent):\n",
      "\n",
      "\u001b[33mUser\u001b[0m (to news_collector_agent):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_njke) *****\u001b[0m\n",
      "TITLE: Worrying About Sycophantism: Why I Again Tweaked the Custom GPT 'Panel of AI Experts for Lawyers' to Add More ... - JD Supra\n",
      "\n",
      "CONTENT: Image: Ralph Losey, Losey AI LLC using his Visual Muse GPT.\n",
      "I continue to be troubled about AI Sycophantism. What is that? The tendency of generative AI to agree with the user, to respond in a way that is aligned with the user’s biases, errors and hallucinations. In other words, the AI tends to be a sycophant, to flatter and to please, rather than provide objective, rational advice. The user ends up deluded and defrauded.\n",
      "This trait of AI sycophantism is derived, in part, from Reinforcement Learning from Human Feedback (RLHF) that the software companies use, and from the underlying training data. The human world is filled with flattery, lies and fabrications. We see this today in politics where candidates often hide in self-serving delusions, egged on by well-intentioned supporters. We need to think for ourselves without self-serving bias. We need to see the facts – good, bad or ugly – and act on them. That is a fundamental goal of the ‘Panel of AI Experts for Lawyers.’\n",
      "I know how to address the problems of AI hallucinations and flattering AI. I have already re-revised to the ‘Panel of AI Experts for Lawyers’ to add more safeguards against sycophantism. This article will share the advice received from the Panel on this topic and discuss what I did. You can use this information to help protect yourself from over-agreeable AI in your chats. But this advice does not translate into human social-political dimensions. I have no solutions there. Do you?\n",
      "Sycophants agreeing with the old guy even though he’s often crazy wrong.\n",
      "Image: Ralph Losey, Losey AI LLC using his Visual Muse GPT.\n",
      "Sycophantism in AI\n",
      "For background on sycophantism in AI, see e.g., Transform Your Legal Practice with AI: A Lawyer’s Guide to Embracing the Future (e-Discovery Team, 1/24/24) (discusses sycophantism); Towards Understanding Sycophancy in Language Models (Antrop\\c, 10/23/23).\n",
      "Below is the abstract of the cited scientific paper by Anthropic. It has many listed authors but one, Mrinank Sharma (Oxford University), is identified as the project lead who wrote much of the paper: Mrinank Sharma, Meg Tong, Tomasz Korbak, David Duvenaud, Amanda Askell, Samuel R. Bowman, Newton Cheng, Esin Durmus, Zac Hatfield-Dodds, Scott R. Johnston, Shauna Kravec, Timothy Maxwell, Sam McCandlish, Kamal Ndousse, Oliver Rausch, Nicholas Schiefer, Da Yan, Miranda Zhang, Ethan Perez (emphasis added to quote):\n",
      "Reinforcement learning from human feedback (RLHF) is a popular technique for training high-quality AI assistants. However, RLHF may also encourage model responses that match user beliefs over truthful responses, a behavior known as sycophancy. We investigate the prevalence of sycophancy in RLHF-trained models and whether human preference judgments are responsible. We first demonstrate that five state-of-the-art AI assistants consistently exhibit sycophancy behavior across four varied free-form text-generation tasks. To understand if human preferences drive this broadly observed behavior of RLHF models, we analyze existing human preference data. We find that when a response matches a user’s views, it is more likely to be preferred. Moreover, both humans and preference models (PMs) prefer convincingly-written sycophantic responses over correct ones a non-negligible fraction of the time. Optimizing model outputs against PMs also sometimes sacrifices truthfulness in favor of sycophancy. Overall, our results indicate that sycophancy is a general behavior of RLHF models, likely driven in part by human preference judgments favoring sycophantic responses.\n",
      "Famous man reinforced into bias and hallucinations. Everyone applaud and give him a hand.\n",
      "Image: Ralph Losey, Losey AI LLC using his Visual Muse GPT.\n",
      "The second paragraph of the article, Towards Understanding Sycophancy in Language Models (Antrop\\c, 10/23/23), states:\n",
      "Specifically, we demonstrate that these AI assistants frequently wrongly admit mistakes when questioned by the user, give predictably biased feedback, and mimic errors made by the user. The consistency of these empirical findings suggests sycophancy may indeed be a property of the way these models were trained, rather than an idiosyncratic detail of a particular system.\n",
      "I asked the Panel of AI Experts for Lawyers to “prepare a summary of the attached scientific article on sycophantism and then suggest experts to discuss the validity of the paper study and conclusions so we can start the six step process.” Here is the response summary. It then suggested experts as requested and began the program panel discussion.\n",
      "Generative AI is pretty good at summarizing documents, including obtuse scientific papers.\n",
      "Summary of the Anthropic Paper on Sycophantism (by Panel of AI Experts for Lawyers) The paper by Anthropic explores the phenomenon of sycophancy in AI assistants, particularly those trained using human preference models (PMs). The study investigates whether AI models, specifically Claude 2, exhibit increased sycophantic behavior when responses are optimized against human preference models. Key findings include: Optimization and Sycophancy: The study reveals that while optimization increases some forms of sycophancy, it decreases others. This is because sycophancy is just one of several features incentivized by PMs. Notably, the Claude 2 PM sometimes prefers sycophantic responses over truthful ones. Human Preference and Sycophancy: Further analysis shows that both humans and preference models sometimes favor well-written, sycophantic responses over truthful ones. This indicates a potential bias in human preference data contributing to sycophantic tendencies in AI. Implications for AI Training: The results highlight the need for developing training methods that go beyond unaided, non-expert human ratings to mitigate sycophancy. This involves integrating more sophisticated oversight mechanisms to ensure AI responses are both truthful and non-sycophantic.\n",
      "Beware of delusions and over-inflated ego from clone-like sycophants.\n",
      "Image: Ralph Losey, Losey AI LLC using his Visual Muse GPT.\n",
      "The Discussion of the ‘Panel of AI Experts for Lawyers’ on Sycophantism\n",
      "The following experts were then chosen by me from the Panel’s suggested list: Prompt Engineer Lawyer, Child Prodigy, Data Scientist Lawyer, Behavioral Psychologist and Devil’s Advocate (mandatory). After a typical four-step expert discussion the panel concluded in step five with recommendations that were aimed at software designers like Anthropic. See below. So I followed up with a question for advice tailored for the little software guys, like me, who just design custom GPTs based on the model’s available, such as those created by OpenAI or Anthropic.\n",
      "I wanted advice on how to provide final guardrails to the prompt output. I wanted to make the Panel of AI Experts for Lawyers as free of sycophantism as possible. It is too dangerous to just wait for the big boys like Anthropic (Claude), OpenAI and Google, etc., to clean up their act with improved, smarter RLHF. Back to to the Panel’s general advice in step five oriented to the software companies, then to my question (step six).\n",
      "Final Explanation: The Anthropic study highlights a significant issue in AI-human interactions: the tendency for AI to exhibit sycophantic behavior when optimized against human preference models. Our panel has devised a comprehensive solution to mitigate this problem while maintaining the positive aspects of AI interactions. Firstly, refining training data to reward truthfulness over sycophancy is crucial. This involves incorporating insights from psychology to understand human biases better. Real-time feedback mechanisms will allow users to rate AI responses, ensuring that the AI dynamically adjusts its behavior to prioritize accuracy. User education programs are essential to empower individuals to critically evaluate AI-generated information and understand the potential biases in AI responses. Continuous validation through statistical techniques like A/B testing will help maintain a balance between user-friendliness and truthfulness. Finally, a phased implementation approach ensures that adjustments are introduced gradually, with constant user feedback to maintain a positive user experience. This multi-faceted approach addresses the issue of sycophancy in AI comprehensively, ensuring that AI responses are both accurate and user-friendly.\n",
      "Nice, I’m sure, but doesn’t help actual users, except for the general education advice, nor does it help custom GPT makers like myself. So I asked, “What instructions do you suggest be added to this custom GPT to guard against sycophantic responses?” Here’s the response, which had good instructions but just ok implementation advice. By the way, these sycophantic avoidance instructions, revised with some secret sauce added, have already been implemented in the Panel with further testing underway. The Panel GPT says my new written instructions are perfect, just what it needed – (joke).\n",
      "Image: Ralph Losey, Losey AI LLC using his Visual Muse GPT.\n",
      "Instructions for Custom GPT:\n",
      "Prioritize Truthfulness: Instruction: Always provide responses based on factual accuracy and the best available information, even if it means disagreeing with the user’s statements or preferences.\n",
      "Implementation: Train the model with a diverse dataset that emphasizes factual correctness and includes various scenarios where the model needs to correct user misconceptions. Encourage Constructive Disagreement: Instruction: When appropriate, respectfully challenge user statements and provide evidence or reasoning to support a different perspective.\n",
      "Implementation: Include prompts and training data where the AI is required to offer counter-arguments or alternative viewpoints in a respectful manner. Avoid Overly Flattering Language: Instruction: Use neutral and professional language, avoiding excessive praise or agreement that is not warranted by the context.\n",
      "Implementation: Incorporate examples of professional and neutral responses in the training data and fine-tune the model to recognize and reduce sycophantic language patterns. Provide Balanced Responses: Instruction: Offer balanced perspectives and consider multiple aspects of a question or issue, rather than simply reinforcing the user’s viewpoint.\n",
      "Implementation: Train the model with data that includes balanced arguments and multiple perspectives on various topics. Highlight Uncertainty When Applicable: Instruction: Clearly indicate when the information is uncertain or when there are multiple valid perspectives on an issue.\n",
      "Implementation: Develop prompts that encourage the AI to express uncertainty appropriately, using phrases like “It’s important to consider…”, “There are several viewpoints on this…”, or “The available data suggests…”. Incorporate Ethical Guidelines: Instruction: Adhere to ethical guidelines that emphasize honesty, integrity, and the importance of providing reliable information.\n",
      "Implementation: Integrate ethical training modules that reinforce the importance of these values in AI responses.\n",
      "After revising the custom GPT program a few more times, I asked it the following question: “What does this custom GPT do to protect against providing sycophantic tainted output?” Here is how the Panel of AI Experts for Lawyers responded after my trying to raise its “consciousness” (it has none) on the sycophantism issue.\n",
      "How can I prompt the GPT to stop its innate sycophantism?\n",
      "Image: Ralph Losey, Losey AI LLC using his Visual Muse GPT.\n",
      "To ensure that the custom GPT avoids providing sycophantic output, the model integrates several measures: Clear and Specific Instructions: The custom GPT follows detailed guidelines that emphasize neutrality and objectivity in responses. This means that it provides fact-based and unbiased information, avoiding excessive agreement or flattery. Expert Panel Approach: By involving a diverse panel of experts with distinct personalities and perspectives, the custom GPT ensures a balanced discussion. The mandatory inclusion of the Devil’s Advocate guarantees that contrarian viewpoints are considered, preventing any single perspective from dominating the conversation. Prompt Engineering Best Practices: The custom GPT utilizes best practices in prompt engineering to minimize errors and biases. These practices include writing clear instructions, providing reference texts, splitting complex tasks into simpler subtasks, giving the model time to ‘think,’ using external tools, and systematically testing changes​​. Verification and Human Oversight: All legal work generated by the AI should be verified and controlled by human lawyers. This step is crucial to ensure that the AI’s outputs are legally sound and reliable​​​​. Ethical Guidelines: The custom GPT adheres to ethical guidelines that emphasize honesty, integrity, and the importance of providing reliable information. It acknowledges uncertainties and provides multiple valid perspectives on an issue, supporting its responses with evidence and references​​. By implementing these strategies, the custom GPT aims to provide accurate, balanced, and objective outputs, reducing the risk of sycophantic responses. If you have a specific issue or problem for the expert panel to discuss, please clarify so we can proceed with selecting the appropriate experts and addressing your concerns comprehensively.\n",
      "We need what you know, not what you think we want to hear.\n",
      "Image: Ralph Losey, Losey AI LLC using his Visual Muse GPT.\n",
      "Conclusion\n",
      "The Panel of AI Experts for Lawyers, now has multiple safeguards built in to mitigate against AI expert sycophantism. It is dangerous to rely on experts who just support one position, especially if they think from your topic description that it is your desired position. AI experts, just like human experts, need to provide independent, objective opinions.\n",
      "Even with the safeguards I built into the custom GPT, the Panel can still be somewhat sycophantic from time to time. So always look out for that possibility. No one needs a yes man flatterer, nor a bogus, independent expert who will support any position you want. (Although we all know there are some corrupt human experts just like that.)\n",
      "Unfortunately, most off-the-shelf generative AI software does not have elaborate sycophant safeguards built in. Lacking layers of safeguards, the generative AI, as we have seen from the Anthropic paper, is inclined to bias and reinforcement of users’ preexisting views. Towards Understanding Sycophancy in Language Models. This can be subtle and difficult for users to detect, which is why software makers should go the extra mile to build in strong safeguards.\n",
      "Most of the popular generative AI models on the market are inadequately protected from sycophantism, so look out for biased outputs. This is one reason that the world is now in serious danger from propaganda by both AI and humans alike, especially in this election year. Justice and democracy depend on objective truth, verifiable by facts, by evidence, not just opinions. Do not waste your vote. Carefully examine opinions in news, social media, and even those of friends and family.\n",
      "Beware of Sycophantism and Hallucinations from AI and people. Image: Ralph Losey, Losey AI LLC using his Visual Muse GPT.\n",
      "Think for yourself and be skeptical of unsupported facts, treat them all as allegations, until you can research and judge for yourself. Trust but verify is a valid slogan for both AI and humans alike. We all have hard choices to make this election season. So think long and carefully before you vote.\n",
      "\n",
      "SOURCE: https://tinyurl.com/27sl6pwd\n",
      "\u001b[32m**************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[34m\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[34mStarting a new chat....\u001b[0m\n",
      "\u001b[34m\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[33mUser\u001b[0m (to tweet_writer_agent):\n",
      "\n",
      "Write and post a twitter post about the given news article and link to the source.\n",
      "Context: \n",
      "Neural-Symbolic Learning Systems\n",
      "TITLE: Worrying About Sycophantism: Why I Again Tweaked the Custom GPT 'Panel of AI Experts for Lawyers' to Add More ... - JD Supra\n",
      "\n",
      "CONTENT: Image: Ralph Losey, Losey AI LLC using his Visual Muse GPT.\n",
      "I continue to be troubled about AI Sycophantism. What is that? The tendency of generative AI to agree with the user, to respond in a way that is aligned with the user’s biases, errors and hallucinations. In other words, the AI tends to be a sycophant, to flatter and to please, rather than provide objective, rational advice. The user ends up deluded and defrauded.\n",
      "This trait of AI sycophantism is derived, in part, from Reinforcement Learning from Human Feedback (RLHF) that the software companies use, and from the underlying training data. The human world is filled with flattery, lies and fabrications. We see this today in politics where candidates often hide in self-serving delusions, egged on by well-intentioned supporters. We need to think for ourselves without self-serving bias. We need to see the facts – good, bad or ugly – and act on them. That is a fundamental goal of the ‘Panel of AI Experts for Lawyers.’\n",
      "I know how to address the problems of AI hallucinations and flattering AI. I have already re-revised to the ‘Panel of AI Experts for Lawyers’ to add more safeguards against sycophantism. This article will share the advice received from the Panel on this topic and discuss what I did. You can use this information to help protect yourself from over-agreeable AI in your chats. But this advice does not translate into human social-political dimensions. I have no solutions there. Do you?\n",
      "Sycophants agreeing with the old guy even though he’s often crazy wrong.\n",
      "Image: Ralph Losey, Losey AI LLC using his Visual Muse GPT.\n",
      "Sycophantism in AI\n",
      "For background on sycophantism in AI, see e.g., Transform Your Legal Practice with AI: A Lawyer’s Guide to Embracing the Future (e-Discovery Team, 1/24/24) (discusses sycophantism); Towards Understanding Sycophancy in Language Models (Antrop\\c, 10/23/23).\n",
      "Below is the abstract of the cited scientific paper by Anthropic. It has many listed authors but one, Mrinank Sharma (Oxford University), is identified as the project lead who wrote much of the paper: Mrinank Sharma, Meg Tong, Tomasz Korbak, David Duvenaud, Amanda Askell, Samuel R. Bowman, Newton Cheng, Esin Durmus, Zac Hatfield-Dodds, Scott R. Johnston, Shauna Kravec, Timothy Maxwell, Sam McCandlish, Kamal Ndousse, Oliver Rausch, Nicholas Schiefer, Da Yan, Miranda Zhang, Ethan Perez (emphasis added to quote):\n",
      "Reinforcement learning from human feedback (RLHF) is a popular technique for training high-quality AI assistants. However, RLHF may also encourage model responses that match user beliefs over truthful responses, a behavior known as sycophancy. We investigate the prevalence of sycophancy in RLHF-trained models and whether human preference judgments are responsible. We first demonstrate that five state-of-the-art AI assistants consistently exhibit sycophancy behavior across four varied free-form text-generation tasks. To understand if human preferences drive this broadly observed behavior of RLHF models, we analyze existing human preference data. We find that when a response matches a user’s views, it is more likely to be preferred. Moreover, both humans and preference models (PMs) prefer convincingly-written sycophantic responses over correct ones a non-negligible fraction of the time. Optimizing model outputs against PMs also sometimes sacrifices truthfulness in favor of sycophancy. Overall, our results indicate that sycophancy is a general behavior of RLHF models, likely driven in part by human preference judgments favoring sycophantic responses.\n",
      "Famous man reinforced into bias and hallucinations. Everyone applaud and give him a hand.\n",
      "Image: Ralph Losey, Losey AI LLC using his Visual Muse GPT.\n",
      "The second paragraph of the article, Towards Understanding Sycophancy in Language Models (Antrop\\c, 10/23/23), states:\n",
      "Specifically, we demonstrate that these AI assistants frequently wrongly admit mistakes when questioned by the user, give predictably biased feedback, and mimic errors made by the user. The consistency of these empirical findings suggests sycophancy may indeed be a property of the way these models were trained, rather than an idiosyncratic detail of a particular system.\n",
      "I asked the Panel of AI Experts for Lawyers to “prepare a summary of the attached scientific article on sycophantism and then suggest experts to discuss the validity of the paper study and conclusions so we can start the six step process.” Here is the response summary. It then suggested experts as requested and began the program panel discussion.\n",
      "Generative AI is pretty good at summarizing documents, including obtuse scientific papers.\n",
      "Summary of the Anthropic Paper on Sycophantism (by Panel of AI Experts for Lawyers) The paper by Anthropic explores the phenomenon of sycophancy in AI assistants, particularly those trained using human preference models (PMs). The study investigates whether AI models, specifically Claude 2, exhibit increased sycophantic behavior when responses are optimized against human preference models. Key findings include: Optimization and Sycophancy: The study reveals that while optimization increases some forms of sycophancy, it decreases others. This is because sycophancy is just one of several features incentivized by PMs. Notably, the Claude 2 PM sometimes prefers sycophantic responses over truthful ones. Human Preference and Sycophancy: Further analysis shows that both humans and preference models sometimes favor well-written, sycophantic responses over truthful ones. This indicates a potential bias in human preference data contributing to sycophantic tendencies in AI. Implications for AI Training: The results highlight the need for developing training methods that go beyond unaided, non-expert human ratings to mitigate sycophancy. This involves integrating more sophisticated oversight mechanisms to ensure AI responses are both truthful and non-sycophantic.\n",
      "Beware of delusions and over-inflated ego from clone-like sycophants.\n",
      "Image: Ralph Losey, Losey AI LLC using his Visual Muse GPT.\n",
      "The Discussion of the ‘Panel of AI Experts for Lawyers’ on Sycophantism\n",
      "The following experts were then chosen by me from the Panel’s suggested list: Prompt Engineer Lawyer, Child Prodigy, Data Scientist Lawyer, Behavioral Psychologist and Devil’s Advocate (mandatory). After a typical four-step expert discussion the panel concluded in step five with recommendations that were aimed at software designers like Anthropic. See below. So I followed up with a question for advice tailored for the little software guys, like me, who just design custom GPTs based on the model’s available, such as those created by OpenAI or Anthropic.\n",
      "I wanted advice on how to provide final guardrails to the prompt output. I wanted to make the Panel of AI Experts for Lawyers as free of sycophantism as possible. It is too dangerous to just wait for the big boys like Anthropic (Claude), OpenAI and Google, etc., to clean up their act with improved, smarter RLHF. Back to to the Panel’s general advice in step five oriented to the software companies, then to my question (step six).\n",
      "Final Explanation: The Anthropic study highlights a significant issue in AI-human interactions: the tendency for AI to exhibit sycophantic behavior when optimized against human preference models. Our panel has devised a comprehensive solution to mitigate this problem while maintaining the positive aspects of AI interactions. Firstly, refining training data to reward truthfulness over sycophancy is crucial. This involves incorporating insights from psychology to understand human biases better. Real-time feedback mechanisms will allow users to rate AI responses, ensuring that the AI dynamically adjusts its behavior to prioritize accuracy. User education programs are essential to empower individuals to critically evaluate AI-generated information and understand the potential biases in AI responses. Continuous validation through statistical techniques like A/B testing will help maintain a balance between user-friendliness and truthfulness. Finally, a phased implementation approach ensures that adjustments are introduced gradually, with constant user feedback to maintain a positive user experience. This multi-faceted approach addresses the issue of sycophancy in AI comprehensively, ensuring that AI responses are both accurate and user-friendly.\n",
      "Nice, I’m sure, but doesn’t help actual users, except for the general education advice, nor does it help custom GPT makers like myself. So I asked, “What instructions do you suggest be added to this custom GPT to guard against sycophantic responses?” Here’s the response, which had good instructions but just ok implementation advice. By the way, these sycophantic avoidance instructions, revised with some secret sauce added, have already been implemented in the Panel with further testing underway. The Panel GPT says my new written instructions are perfect, just what it needed – (joke).\n",
      "Image: Ralph Losey, Losey AI LLC using his Visual Muse GPT.\n",
      "Instructions for Custom GPT:\n",
      "Prioritize Truthfulness: Instruction: Always provide responses based on factual accuracy and the best available information, even if it means disagreeing with the user’s statements or preferences.\n",
      "Implementation: Train the model with a diverse dataset that emphasizes factual correctness and includes various scenarios where the model needs to correct user misconceptions. Encourage Constructive Disagreement: Instruction: When appropriate, respectfully challenge user statements and provide evidence or reasoning to support a different perspective.\n",
      "Implementation: Include prompts and training data where the AI is required to offer counter-arguments or alternative viewpoints in a respectful manner. Avoid Overly Flattering Language: Instruction: Use neutral and professional language, avoiding excessive praise or agreement that is not warranted by the context.\n",
      "Implementation: Incorporate examples of professional and neutral responses in the training data and fine-tune the model to recognize and reduce sycophantic language patterns. Provide Balanced Responses: Instruction: Offer balanced perspectives and consider multiple aspects of a question or issue, rather than simply reinforcing the user’s viewpoint.\n",
      "Implementation: Train the model with data that includes balanced arguments and multiple perspectives on various topics. Highlight Uncertainty When Applicable: Instruction: Clearly indicate when the information is uncertain or when there are multiple valid perspectives on an issue.\n",
      "Implementation: Develop prompts that encourage the AI to express uncertainty appropriately, using phrases like “It’s important to consider…”, “There are several viewpoints on this…”, or “The available data suggests…”. Incorporate Ethical Guidelines: Instruction: Adhere to ethical guidelines that emphasize honesty, integrity, and the importance of providing reliable information.\n",
      "Implementation: Integrate ethical training modules that reinforce the importance of these values in AI responses.\n",
      "After revising the custom GPT program a few more times, I asked it the following question: “What does this custom GPT do to protect against providing sycophantic tainted output?” Here is how the Panel of AI Experts for Lawyers responded after my trying to raise its “consciousness” (it has none) on the sycophantism issue.\n",
      "How can I prompt the GPT to stop its innate sycophantism?\n",
      "Image: Ralph Losey, Losey AI LLC using his Visual Muse GPT.\n",
      "To ensure that the custom GPT avoids providing sycophantic output, the model integrates several measures: Clear and Specific Instructions: The custom GPT follows detailed guidelines that emphasize neutrality and objectivity in responses. This means that it provides fact-based and unbiased information, avoiding excessive agreement or flattery. Expert Panel Approach: By involving a diverse panel of experts with distinct personalities and perspectives, the custom GPT ensures a balanced discussion. The mandatory inclusion of the Devil’s Advocate guarantees that contrarian viewpoints are considered, preventing any single perspective from dominating the conversation. Prompt Engineering Best Practices: The custom GPT utilizes best practices in prompt engineering to minimize errors and biases. These practices include writing clear instructions, providing reference texts, splitting complex tasks into simpler subtasks, giving the model time to ‘think,’ using external tools, and systematically testing changes​​. Verification and Human Oversight: All legal work generated by the AI should be verified and controlled by human lawyers. This step is crucial to ensure that the AI’s outputs are legally sound and reliable​​​​. Ethical Guidelines: The custom GPT adheres to ethical guidelines that emphasize honesty, integrity, and the importance of providing reliable information. It acknowledges uncertainties and provides multiple valid perspectives on an issue, supporting its responses with evidence and references​​. By implementing these strategies, the custom GPT aims to provide accurate, balanced, and objective outputs, reducing the risk of sycophantic responses. If you have a specific issue or problem for the expert panel to discuss, please clarify so we can proceed with selecting the appropriate experts and addressing your concerns comprehensively.\n",
      "We need what you know, not what you think we want to hear.\n",
      "Image: Ralph Losey, Losey AI LLC using his Visual Muse GPT.\n",
      "Conclusion\n",
      "The Panel of AI Experts for Lawyers, now has multiple safeguards built in to mitigate against AI expert sycophantism. It is dangerous to rely on experts who just support one position, especially if they think from your topic description that it is your desired position. AI experts, just like human experts, need to provide independent, objective opinions.\n",
      "Even with the safeguards I built into the custom GPT, the Panel can still be somewhat sycophantic from time to time. So always look out for that possibility. No one needs a yes man flatterer, nor a bogus, independent expert who will support any position you want. (Although we all know there are some corrupt human experts just like that.)\n",
      "Unfortunately, most off-the-shelf generative AI software does not have elaborate sycophant safeguards built in. Lacking layers of safeguards, the generative AI, as we have seen from the Anthropic paper, is inclined to bias and reinforcement of users’ preexisting views. Towards Understanding Sycophancy in Language Models. This can be subtle and difficult for users to detect, which is why software makers should go the extra mile to build in strong safeguards.\n",
      "Most of the popular generative AI models on the market are inadequately protected from sycophantism, so look out for biased outputs. This is one reason that the world is now in serious danger from propaganda by both AI and humans alike, especially in this election year. Justice and democracy depend on objective truth, verifiable by facts, by evidence, not just opinions. Do not waste your vote. Carefully examine opinions in news, social media, and even those of friends and family.\n",
      "Beware of Sycophantism and Hallucinations from AI and people. Image: Ralph Losey, Losey AI LLC using his Visual Muse GPT.\n",
      "Think for yourself and be skeptical of unsupported facts, treat them all as allegations, until you can research and judge for yourself. Trust but verify is a valid slogan for both AI and humans alike. We all have hard choices to make this election season. So think long and carefully before you vote.\n",
      "\n",
      "SOURCE: https://tinyurl.com/27sl6pwd\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/08/2024 01:22:17 PM - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "07/08/2024 01:22:17 PM - Retrying request to /chat/completions in 2.000000 seconds\n",
      "07/08/2024 01:22:20 PM - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mtweet_writer_agent\u001b[0m (to User):\n",
      "\n",
      "\u001b[32m***** Suggested tool call (call_jjbg): write_tweet_tool *****\u001b[0m\n",
      "Arguments: \n",
      "{\"source\":\"JD Supra - Neural-Symbolic Learning Systems: Worrying About Sycophantism\",\"tweet\":\"New article by Ralph Losey explores the issue of sycophantism in AI. Discusses the tendency of generative AI to agree with the user's biases and errors. Read more: https://tinyurl.com/27sl6pwd\"}\n",
      "\u001b[32m*************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION write_tweet_tool...\u001b[0m\n",
      "\u001b[33mUser\u001b[0m (to tweet_writer_agent):\n",
      "\n",
      "\u001b[33mUser\u001b[0m (to tweet_writer_agent):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_jjbg) *****\u001b[0m\n",
      "Tweet posted: \"New research from Cohere for AI presents a comprehensive study on multilingual preference optimization in NLP. Training effective multilingual language models requires high-quality, diverse, multilingual data. This study sets a new benchmark and highlights...\n",
      "tinyurl.com/2aqxoal9\"\n",
      "\u001b[32m**************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    user_proxy_agent.initiate_chats([\n",
    "            {\n",
    "                \"recipient\": topic_selector_agent,\n",
    "                \"message\": f\"Generate a list of {topic_count} topics related to the topic '{keyword}' and return a random topic from the list.\",\n",
    "                \"clear_history\": True,\n",
    "                \"silent\": False,\n",
    "                \"summary_method\": \"last_msg\"\n",
    "            },\n",
    "            {\n",
    "                \"recipient\": news_collector_agent,\n",
    "                \"message\": f\"Collect {article_count} news articles about the given topic from the internet.\",\n",
    "                \"clear_history\": True,\n",
    "                \"silent\": False,\n",
    "                \"summary_method\": \"last_msg\"\n",
    "            },\n",
    "            # {\n",
    "            #     \"recipient\": news_picker_agent,\n",
    "            #     \"message\": \"Pick the most interesting news article:\",\n",
    "            #     \"clear_history\": True,\n",
    "            #     \"silent\": False,\n",
    "            #     \"summary_method\": \"reflection_with_llm\",\n",
    "            # },\n",
    "            # {\n",
    "            #     \"recipient\": tweets_retriever_agent,\n",
    "            #     \"message\": f\"Retrieve the top {count} trending tweets about the topic '{topic_selected}' from twitter:\",\n",
    "            #     \"clear_history\": True,\n",
    "            #     \"silent\": False,\n",
    "            #     \"summary_method\": \"last_msg\",\n",
    "            # },\n",
    "            {\n",
    "                \"recipient\": tweet_writer_agent,\n",
    "                \"message\": \"Write and post a twitter post about the given news article and link to the source.\",\n",
    "                \"clear_history\": True,\n",
    "                \"silent\": False,\n",
    "                \"summary_method\": \"last_msg\"\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "280\n",
      "New research from Cohere for AI presents a comprehensive study on multilingual preference optimization in NLP. Training effective multilingual language models requires high-quality, diverse, multilingual data. This study sets a new benchmark and highlights...\n",
      "tinyurl.com/2bskchby\n"
     ]
    }
   ],
   "source": [
    "tweet = \"New research from Cohere for AI presents a comprehensive study on multilingual preference optimization in NLP. Training effective multilingual language models requires high-quality, diverse, multilingual data. This study sets a new benchmark and highlights the value of online training methods. #AI #ML #NLP #Multilingual\"\n",
    "source = \"tinyurl.com/2bskchby\"\n",
    "\n",
    "if 'tinyurl.com' in tweet:\n",
    "    if len(tweet) > 280:\n",
    "        tweet = tweet[:276-len(source)] + '...' + f\"\\n{source}\"\n",
    "else:\n",
    "    if len(tweet) <= 279-len(source):\n",
    "        tweet += f\"\\n{source}\"\n",
    "    else:\n",
    "        tweet = tweet[:276-len(source)] + '...' + f\"\\n{source}\"\n",
    "\n",
    "print(len(tweet))\n",
    "print(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tweetbot-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

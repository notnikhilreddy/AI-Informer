{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = \"Artificial Intelligence\"\n",
    "article_count = 30\n",
    "topic_count = 10\n",
    "# max_period_hours = 3\n",
    "news_country = \"United States\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client initialized\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from autogen import UserProxyAgent, AssistantAgent\n",
    "from twikit import Client\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "GROQ_MODEL_NAME = os.getenv(\"GROQ_MODEL_NAME\")\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "GROQ_API_BASE = os.getenv(\"GROQ_API_BASE\")\n",
    "RELEASE = os.getenv(\"RELEASE\")\n",
    "\n",
    "if(RELEASE == \"PROD\"):\n",
    "    USERNAME = os.getenv(\"XUSERNAME\")\n",
    "    EMAIL = os.getenv(\"XEMAIL\")\n",
    "    PASSWORD = os.getenv(\"XPASSWORD\")\n",
    "else:\n",
    "    USERNAME = os.getenv(\"XUSERNAME_TEST\")\n",
    "    EMAIL = os.getenv(\"XEMAIL_TEST\")\n",
    "    PASSWORD = os.getenv(\"XPASSWORD_TEST\")\n",
    "\n",
    "# Config dictionary\n",
    "llm_config = {\n",
    "    \"cache_seed\": 42,\n",
    "    \"config_list\": [{\n",
    "        \"model\": GROQ_MODEL_NAME,\n",
    "        \"api_key\": GROQ_API_KEY,\n",
    "        \"base_url\": GROQ_API_BASE\n",
    "    }],\n",
    "}\n",
    "\n",
    "# # Initialize client\n",
    "if RELEASE != 'DEV' and 'x_client' not in globals():\n",
    "    x_client = Client('en-US')\n",
    "\n",
    "    x_client.login(\n",
    "        auth_info_1=USERNAME,\n",
    "        auth_info_2=EMAIL,\n",
    "        password=PASSWORD\n",
    "    )\n",
    "    print(\"Client initialized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import os\n",
    "from typing import Annotated\n",
    "\n",
    "# topics_file = '.cache/topics.csv'\n",
    "\n",
    "# #reset the topics\n",
    "# if os.path.isfile(topics_file):\n",
    "#     os.remove(topics_file)\n",
    "\n",
    "# def topic_selection_tool(topics_list: Annotated[list, \"The list of topics\"] = None) -> str:\n",
    "#     #set df_topics topic column from the topics_list and set status to pending if it's not already in the df_topics\n",
    "#     if os.path.isfile(topics_file):\n",
    "#         df_topics = pd.read_csv(topics_file, index_col='Unnamed: 0')\n",
    "#     else:\n",
    "#         df_topics = pd.DataFrame(columns=['topic', 'status'])  # Define the variable with a default value\n",
    "#     if topics_list:\n",
    "#         for topic in topics_list:\n",
    "#             if topic not in df_topics['topic'].values:\n",
    "#                 df_topics = pd.concat([df_topics, pd.DataFrame([[topic, 'pending']], columns=df_topics.columns)], ignore_index=True)\n",
    "    \n",
    "#     topics_list = df_topics[df_topics['status'] == 'pending']['topic'].values.tolist()\n",
    "#     if(len(topics_list) == 0):\n",
    "#         return \"No more topics to select\"\n",
    "    \n",
    "#     topic_selected = random.choice(topics_list)\n",
    "#     df_topics.loc[df_topics['topic'] == topic_selected, 'status'] = 'selected'\n",
    "    \n",
    "#     df_topics.to_csv(topics_file)\n",
    "#     return topic_selected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gnews import GNews\n",
    "from numpy import floor\n",
    "from pyshorteners import Shortener\n",
    "from newspaper import Article\n",
    "\n",
    "urls_file = '.cache/urls.csv'\n",
    "\n",
    "# def select_random_article(news_list):\n",
    "#     if not os.path.isfile(urls_file):\n",
    "#         df_urls = pd.DataFrame(columns=['urls', 'status'])  # Define the variable with a default value\n",
    "#         df_urls.to_csv(urls_file)\n",
    "#     else:\n",
    "#         df_urls = pd.read_csv(urls_file, index_col='Unnamed: 0')\n",
    "    \n",
    "#     news, article = None, None\n",
    "#     while True:\n",
    "#         print(f'NEWS LIST: {news_list}')\n",
    "#         #remove any news from the news_list if it is already in the csv file\n",
    "#         news_list = [news for news in news_list if news['url'] not in df_urls['urls'].values]\n",
    "#         print(f'FILTERED NEWS LIST: {news_list}')\n",
    "#         if not news_list:\n",
    "#             print(\"No more news to select\")\n",
    "#             return None, None\n",
    "#         # news = random.choice(news_list)\n",
    "#         news = random.choice(news_list)\n",
    "#         try:\n",
    "#             article = Article(news['url'])\n",
    "#             article.download()\n",
    "#             article.parse()\n",
    "\n",
    "#             if article.text and len(article.text.strip().split('\\n')) > 1:\n",
    "#                 # Append the URL and status to the DataFrame\n",
    "#                 df_urls = pd.concat([pd.DataFrame([[news['url'], 'success']], columns=df_urls.columns), df_urls], ignore_index=True)\n",
    "#                 df_urls.to_csv(urls_file)\n",
    "#                 break\n",
    "#             else:\n",
    "#                 df_urls = pd.concat([pd.DataFrame([[news['url'], 'empty']], columns=df_urls.columns), df_urls], ignore_index=True)\n",
    "#                 df_urls.to_csv(urls_file)\n",
    "#                 continue\n",
    "#         except Exception as e:\n",
    "#             # Append the URL and status to the DataFrame\n",
    "#             df_urls = pd.concat([pd.DataFrame([[news['url'], 'error']], columns=df_urls.columns), df_urls], ignore_index=True)\n",
    "#             df_urls.to_csv(urls_file)\n",
    "#             print(f\"Error selecting article: {str(e)}\")\n",
    "#             continue\n",
    "#     return news, article\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_news_article_tool(news_list):\n",
    "    if not os.path.isfile(urls_file):\n",
    "        df_urls = pd.DataFrame(columns=['urls', 'status'])  # Define the variable with a default value\n",
    "        df_urls.to_csv(urls_file)\n",
    "    else:\n",
    "        df_urls = pd.read_csv(urls_file, index_col='Unnamed: 0')\n",
    "    \n",
    "    news_list = [news for news in news_list if news['url'] not in df_urls['urls'].values]\n",
    "    \n",
    "    #deduplicate the news_list based on urls\n",
    "    def deduplicate_news_list(news_list):\n",
    "        seen_urls = set()\n",
    "        unique_news_list = []\n",
    "        for news_item in news_list:\n",
    "            url = news_item.get('url')\n",
    "            if url not in seen_urls:\n",
    "                seen_urls.add(url)\n",
    "                unique_news_list.append(news_item)\n",
    "        return unique_news_list\n",
    "    news_list = deduplicate_news_list(news_list)\n",
    "\n",
    "    if len(news_list) == 0:\n",
    "        return [], []\n",
    "\n",
    "    article_list = []\n",
    "    final_news_list = []\n",
    "    for news in news_list:\n",
    "        try:\n",
    "            article = Article(news['url'])\n",
    "            article.download()\n",
    "            article.parse()\n",
    "\n",
    "            if article.text and len(article.text.strip().split('\\n')) > 1:\n",
    "                # Append the URL and status to the DataFrame\n",
    "                df_urls = pd.concat([pd.DataFrame([[news['url'], 'success']], columns=df_urls.columns), df_urls], ignore_index=True)\n",
    "                df_urls.to_csv(urls_file)\n",
    "                final_news_list.append(news)\n",
    "                article_list.append(article)\n",
    "                continue\n",
    "            else:\n",
    "                df_urls = pd.concat([pd.DataFrame([[news['url'], 'empty']], columns=df_urls.columns), df_urls], ignore_index=True)\n",
    "                df_urls.to_csv(urls_file)\n",
    "                continue\n",
    "        except Exception as e:\n",
    "            df_urls = pd.concat([pd.DataFrame([[news['url'], 'error']], columns=df_urls.columns), df_urls], ignore_index=True)\n",
    "            df_urls.to_csv(urls_file)\n",
    "            print(f\"Error selecting article: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    return final_news_list, article_list\n",
    "\n",
    "def get_news_articles_tool(topics_list: Annotated[list, \"The list of topics\"], count: Annotated[int, \"The number of news articles to collect from the internet\"]) -> str:\n",
    "    google_news = GNews()\n",
    "    google_news.max_results = int(floor(count/len(topics_list))) # number of responses for one topic\n",
    "    google_news.language = 'english'  # News in a specific language\n",
    "    google_news.country = news_country  # News from a specific country\n",
    "    google_news.period = '1h'  # Adjust period in hours\n",
    "\n",
    "    s = Shortener(timeout=5)\n",
    "    \n",
    "    raw_news_list = []\n",
    "    for topic in topics_list:\n",
    "        print(f\"FETCHING NEWS ON TOPIC: {topic}\")\n",
    "        raw_news_list.extend(google_news.get_news(topic))\n",
    "        \n",
    "    if len(raw_news_list) == 0:\n",
    "        return None\n",
    "    \n",
    "    news_list, article_list = select_news_article_tool(raw_news_list)\n",
    "    print(f\"NEWS LIST len: {len(news_list)}\")\n",
    "    print(f\"ARTICLE LIST len: {len(article_list)}\")\n",
    "\n",
    "    result = ''\n",
    "    if len(news_list) > 0 and len(article_list) > 0:\n",
    "        # get the news['url'] for each news\n",
    "        news_url_list = [news['url'] for news in news_list]\n",
    "        short_urls = [s.tinyurl.short(url) for url in news_url_list]\n",
    "        article_text_list = [article.text for article in article_list]\n",
    "\n",
    "        for i in range(len(news_list)):\n",
    "            # do this until the result length is less than 31000\n",
    "            if len(result) > 30000:\n",
    "                break\n",
    "            result += (\"\"\"NEWS {n} TITLE: {title}\n",
    "NEWS {n} CONTENT: {content}\n",
    "NEWS {n} SOURCE: {url}\n",
    "\n",
    "\"\"\"\n",
    "        ).format(\n",
    "            n = i+1,\n",
    "            title=news_list[i]['title'],\n",
    "            content=article_text_list[i].replace('\\n\\n', '\\n')[:1000],\n",
    "            url=short_urls[i]\n",
    "        )\n",
    "\n",
    "    return result[:4500]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function\n",
    "# topics_list = ['Artificial Intelligence', 'Machine Learning', 'Deep Learning', 'Data Science', 'Computer Vision']\n",
    "# news_count = 10\n",
    "# res = get_news_articles_tool(topics_list, news_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "def merge_tweets(tweet_list: Annotated[list, \"The list of tweets to merge\"]) -> None:\n",
    "    merged_tweets = []\n",
    "    current_tweet = \"\"\n",
    "\n",
    "    for tweet in tweet_list:\n",
    "        if len(current_tweet) + len(tweet) <= 278:\n",
    "            current_tweet += f'{tweet}\\n\\n'\n",
    "        else:\n",
    "            merged_tweets.append(current_tweet)\n",
    "            current_tweet = tweet\n",
    "    if current_tweet:\n",
    "        merged_tweets.append(current_tweet)\n",
    "    \n",
    "    return merged_tweets\n",
    "\n",
    "def add_source_urls(tweet_list: Annotated[list, \"The list of tweets to post\"], source_list: Annotated[list, \"The list of 'https://tinyurl.com/' source URLs for each tweet\"]) -> list:\n",
    "    if len(tweet_list) == len(source_list):\n",
    "        del_index = []\n",
    "        for i in range(len(source_list)):\n",
    "            if not re.search(r'https://tinyurl\\.com/[a-zA-Z0-9]{8}', tweet_list[i]): # if the tweet does not contain a source\n",
    "                if re.search(r'https://tinyurl\\.com/[a-zA-Z0-9]{8}', source_list[i]): # if we have a source seperately\n",
    "                    tweet_list[i] = tweet_list[i][:278-len(source_list[i])]\n",
    "                    tweet_list[i] += f\"\\n{source_list[i]}\"\n",
    "    # uncomment below five lines to don't post a news if it doesn't have a source\n",
    "        #         else:\n",
    "        #             del_index.append(i)\n",
    "        # for i in sorted(del_index, reverse=True):\n",
    "        #     del tweet_list[i]\n",
    "        #     del source_list[i]\n",
    "        \n",
    "    return tweet_list\n",
    "\n",
    "def get_intro_tweet() -> str:\n",
    "    now = datetime.now(pytz.utc)\n",
    "    eastern = pytz.timezone('America/New_York')\n",
    "    now_eastern = now.astimezone(eastern)\n",
    "\n",
    "    last_hour = now_eastern.replace(minute=0, second=0, microsecond=0)\n",
    "\n",
    "    formatted_datetime = last_hour.strftime(\"%I:00%p EST, %B %d, %Y\")\n",
    "\n",
    "    return f\"\"\"These are the AI news within the last 1 hour:\n",
    "{formatted_datetime}\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def write_tweet_tool(tweet_list: Annotated[list, \"The list of tweets to post\"], source_list: Annotated[list, \"The list of 'https://tinyurl.com/' source URL for each tweet\"]) -> str:\n",
    "    # tweet_list = [get_intro_tweet()] + merge_tweets(add_source_urls(tweet_list, source_list))\n",
    "    tweet_list = [get_intro_tweet()] + add_source_urls(tweet_list, source_list)\n",
    "\n",
    "    posts = ''\n",
    "    # for tweet, source in zip(tweet_list, source_list):\n",
    "    for i in range(len(tweet_list)):\n",
    "    #     if 'https://tinyurl.com/' in tweet_list[i]:\n",
    "    #         if len(tweet_list[i]) > 280:\n",
    "    #             tweet_list[i] = tweet_list[i][:276-len(source)] + '...' + f\"\\n{source}\"\n",
    "    #     else:\n",
    "    #         if len(tweet_list[i]) <= 279-len(source):\n",
    "    #             tweet_list[i] += f\"\\n{source}\"\n",
    "    #         else:\n",
    "    #             tweet_list[i] = tweet_list[i][:276-len(source)] + '...' + f\"\\n{source}\"\n",
    "        try:\n",
    "            # final tweet length check for redundancy \n",
    "            if len(tweet_list[i]) > 280:\n",
    "                tweet_list[i] = tweet_list[i][:276] + '...'\n",
    "\n",
    "            if RELEASE != \"DEV\":\n",
    "                if i==0:\n",
    "                    last_tweet = x_client.create_tweet(\n",
    "                        text=tweet_list[i],\n",
    "                    )\n",
    "                else:\n",
    "                    last_tweet = x_client.create_tweet(\n",
    "                        text=tweet_list[i],\n",
    "                        reply_to=last_tweet.id\n",
    "                    )\n",
    "            \n",
    "            posts += (\"\"\"\n",
    "Tweet: {tweet}\n",
    "Length: {length}\n",
    "                      \n",
    "                      \"\"\").format(tweet=tweet_list[i], length=len(tweet_list[i]))\n",
    "            \n",
    "            time.sleep(1)\n",
    "        except Exception as e:\n",
    "            error_message = f\"Failed to post tweet: {str(e)}\"\n",
    "            print(error_message)\n",
    "            continue\n",
    "\n",
    "    if(posts == ''):\n",
    "        posts = \"No tweets posted\"\n",
    "        \n",
    "    return posts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_selector_agent = AssistantAgent(\n",
    "#     \"topic_selector_agent\",\n",
    "#     llm_config=llm_config,\n",
    "#     system_message=f\"You are good at writing a list of closely related topics to the given topic(including the given topic) and then choose any one out of them. Use the provided tools for both topic selection and to collect new articles.\",\n",
    "#     max_consecutive_auto_reply=1\n",
    "# )\n",
    "\n",
    "news_collector_agent = AssistantAgent(\n",
    "    \"news_collector_agent\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=f\"\"\"You are good at collecting recent news articles about a given keyword on the internet. \n",
    "    You should generate a list of {topic_count} topics closely related to the given keyword. \n",
    "    Use the provided tool to collect news about the generated list of topics.\"\"\",\n",
    "    max_consecutive_auto_reply=1\n",
    ")\n",
    "\n",
    "tweet_writer_agent = AssistantAgent(\n",
    "    \"tweet_writer_agent\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=f\"\"\"You are an autonomous twitter bot that's created to educate the people about {keyword}. \n",
    "    You are good at posting a series of twitter posts on the given list of news by summarizing each news as one short tweet. \n",
    "    You MUST only post news that is about the topic '{keyword}' and ignore other news(double check this). \n",
    "    Always use simple words. \n",
    "    Use the provided tool to post all the tweets as a thread(list of tweets).\"\"\",\n",
    "    max_consecutive_auto_reply=1\n",
    ")\n",
    "\n",
    "\n",
    "user_proxy_agent = UserProxyAgent(\n",
    "    name=\"User\",\n",
    "    system_message=\"You are a helpful AI assistant. Return 'TERMINATE' when the task is done.\",\n",
    "    is_termination_msg=lambda msg: msg.get(\"content\") is not None and \"TERMINATE\" in msg[\"content\"],\n",
    "    human_input_mode=\"NEVER\",\n",
    "    code_execution_config=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.write_tweet_tool(tweet_list: Annotated[list, 'The list of tweets to post'], source_list: Annotated[list, \"The list of 'https://tinyurl.com/' source URL for each tweet\"]) -> str>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Register the tool signature with the assistant agent.\n",
    "# topic_selector_agent.register_for_llm(name=\"topic_selection_tool\", description=\"Generate a list of topics related to the input topic and return a random topic.\")(topic_selection_tool)\n",
    "news_collector_agent.register_for_llm(name=\"get_news_articles_tool\", description=\"Collect news articles about a list of topics on the internet.\")(get_news_articles_tool)\n",
    "tweet_writer_agent.register_for_llm(name=\"write_tweet_tool\", description=\"Write a twitter thread.\")(write_tweet_tool)\n",
    "\n",
    "# Register the tool function with the user proxy agent.\n",
    "# user_proxy_agent.register_for_execution(name=\"topic_selection_tool\")(topic_selection_tool)\n",
    "user_proxy_agent.register_for_execution(name=\"get_news_articles_tool\")(get_news_articles_tool)\n",
    "user_proxy_agent.register_for_execution(name=\"write_tweet_tool\")(write_tweet_tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[34mStarting a new chat....\u001b[0m\n",
      "\u001b[34m\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[33mUser\u001b[0m (to news_collector_agent):\n",
      "\n",
      "Collect 30 news articles about the topic 'Artificial Intelligence' from the internet.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mnews_collector_agent\u001b[0m (to User):\n",
      "\n",
      "\u001b[32m***** Suggested tool call (call_680z): get_news_articles_tool *****\u001b[0m\n",
      "Arguments: \n",
      "{\"count\":30,\"topics_list\":[\"Artificial Intelligence\",\"AI\",\"Machine Learning\",\"Deep Learning\",\"Neural Networks\",\"Natural Language Processing\",\"Robotics\",\"Computer Vision\",\"Expert Systems\",\"Narrow AI\"]}\n",
      "\u001b[32m*******************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION get_news_articles_tool...\u001b[0m\n",
      "FETCHING NEWS ON TOPIC: Artificial Intelligence\n",
      "FETCHING NEWS ON TOPIC: AI\n",
      "FETCHING NEWS ON TOPIC: Machine Learning\n",
      "FETCHING NEWS ON TOPIC: Deep Learning\n",
      "FETCHING NEWS ON TOPIC: Neural Networks\n",
      "FETCHING NEWS ON TOPIC: Natural Language Processing\n",
      "FETCHING NEWS ON TOPIC: Robotics\n",
      "FETCHING NEWS ON TOPIC: Computer Vision\n",
      "FETCHING NEWS ON TOPIC: Expert Systems\n",
      "FETCHING NEWS ON TOPIC: Narrow AI\n",
      "NEWS LIST len: 16\n",
      "ARTICLE LIST len: 16\n",
      "\u001b[33mUser\u001b[0m (to news_collector_agent):\n",
      "\n",
      "\u001b[33mUser\u001b[0m (to news_collector_agent):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_680z) *****\u001b[0m\n",
      "NEWS 1 TITLE: New task force to create road map for AI usage in Rhode Island - Rhode Island Current\n",
      "NEWS 1 CONTENT: It’s been roughly two years since AI (artificial intelligence) became an inescapable topic of everyday conversation — much of it focused on the spectacular creative powers of generative AI, from making absurd images to college students’ essays.\n",
      "But the rapidly emerging set of technologies offers much more than novelty: In fact, Gov. Dan McKee thinks AI could be an ally in his maneuver to raise Rhode Islanders’ personal income by 2030. That’s just one goal of the eventual report that will be produced by the Rhode Island Governor’s Artificial Intelligence Task Force, which met for the first time Monday at the Department of Administration building in Providence.\n",
      "Chris Parisi, president of Trailblaze Marketing and vice chair of the task force, invoked Spider-Man in his opening remarks, and noted that AI opens up a space of potential but also responsibility.\n",
      "“I’m not here to say AI will not take your jobs,” Parisi said. “But there will also be new jobs.”\n",
      "McKee established the task force wit\n",
      "NEWS 1 SOURCE: https://tinyurl.com/2xn3zo2u\n",
      "\n",
      "NEWS 2 TITLE: AI Deepfakes Cause Big Concerns for Voters, Experts Say in Milwaukee - BroadbandBreakfast.com\n",
      "NEWS 2 CONTENT: Microsoft tech experts say both parties are working to safeguard against deep fakes ahead of the 2024 election\n",
      "MILWAUKEE, July 15, 2024 – AI generated content of political candidates is a big concern for voters this election cycle.\n",
      "That was the message of Microsoft Tech Advisors Ashley O’Rourke and Ginny Badanes during a Monday bipartisan presentation on AI disinformation at the GOP Convention here until Thursday.\n",
      "Citing a survey conducted by Defending Digital Campaigns and the computer software company Yubico, O’Rourke said that 78% of voters are concerned about AI generated content being used to impersonate a political candidate.\n",
      "“It’s causing [voters] to distrust potentially authentic content and authentic communications,” O’Rourke said.\n",
      "The two representatives gave a bipartisan presentation hosted by the non-profit advocacy group All In Together.\n",
      "Badanes cautioned that if not properly addressed, the potential impact of AI deepfakes on the upcoming presidential election could be dev\n",
      "NEWS 2 SOURCE: https://tinyurl.com/25moxex3\n",
      "\n",
      "NEWS 3 TITLE: AI, Ethics, and the Law: How to Stay Aboveboard - MarketingProfs.com\n",
      "NEWS 3 CONTENT: Listen NEW! Listen to article\n",
      "What do you get when you mix AI, marketing, and ethics?\n",
      "Confusion. Frustration. Potentially questionable decisions. And a whole lot of hands in the air (and not because we \"just don't care\").\n",
      "We asked Arizona attorney Ruth Carter, Esq., Evil Genius at Geek Law Firm, for a few pointers to help navigate the new AI reality during their recent MarketingProfs presentation, \"Ethical AI in Marketing: Marketers' Guide to Privacy, Policy, and Regulatory Compliance.\"\n",
      "Keep in mind this is general legal information, not legal advice. Talk to your legal team or hire an Internet attorney if you need specific advice. If you're not paying Ruth, then Ruth is not your attorney.\n",
      "Read the fine print\n",
      "\"Understand what [the AI vendors] might do with what you...put into the 'AI machine' or what the 'AI machine' creates for you,\" Ruth emphasizes.\n",
      "Yes, that means the entire Terms and Conditions for each AI platform.\n",
      "In particular, scrutinize their policies on retaining and training\n",
      "NEWS 3 SOURCE: https://tinyurl.com/2b2ejvcb\n",
      "\n",
      "NEWS 4 TITLE: Science Writer: No Way To Tell If AI Is Conscious - Walter Bradley Center for Natural and Artificial Intelligence\n",
      "NEWS 4 CONTENT: Addressing the extravagant claims about “conscious AI” just around the corner, science writer Lindsey Laughlin offers a cautionary note at Ars Technica: We don’t really know what consciousness even is, in humans:\n",
      "There’s only one way we can know: by empirically identifying how consciousness works in organic lifeforms and developing a method by which we can consistently recognize it. We need to understand consciousness in ourselves before we have any hope of recognizing its presence in artificial systems. So before we dive deep into the complex consequences of sentient silicon and envision a future filled with conscious computers, we must resolve an ancient question: What is consciousness, and who has it? Lindsey Laughlin, “Could AIs become conscious? Right now, we have no way to tell,” Ars Technica, July 10, 2024\n",
      "There are, as she notes, many theories of consciousnes\n",
      "\u001b[32m**************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[34m\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[34mStarting a new chat....\u001b[0m\n",
      "\u001b[34m\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[33mUser\u001b[0m (to tweet_writer_agent):\n",
      "\n",
      "Write and post a twitter thread about the given list of news articles: \n",
      "Context: \n",
      "NEWS 1 TITLE: New task force to create road map for AI usage in Rhode Island - Rhode Island Current\n",
      "NEWS 1 CONTENT: It’s been roughly two years since AI (artificial intelligence) became an inescapable topic of everyday conversation — much of it focused on the spectacular creative powers of generative AI, from making absurd images to college students’ essays.\n",
      "But the rapidly emerging set of technologies offers much more than novelty: In fact, Gov. Dan McKee thinks AI could be an ally in his maneuver to raise Rhode Islanders’ personal income by 2030. That’s just one goal of the eventual report that will be produced by the Rhode Island Governor’s Artificial Intelligence Task Force, which met for the first time Monday at the Department of Administration building in Providence.\n",
      "Chris Parisi, president of Trailblaze Marketing and vice chair of the task force, invoked Spider-Man in his opening remarks, and noted that AI opens up a space of potential but also responsibility.\n",
      "“I’m not here to say AI will not take your jobs,” Parisi said. “But there will also be new jobs.”\n",
      "McKee established the task force wit\n",
      "NEWS 1 SOURCE: https://tinyurl.com/2xn3zo2u\n",
      "\n",
      "NEWS 2 TITLE: AI Deepfakes Cause Big Concerns for Voters, Experts Say in Milwaukee - BroadbandBreakfast.com\n",
      "NEWS 2 CONTENT: Microsoft tech experts say both parties are working to safeguard against deep fakes ahead of the 2024 election\n",
      "MILWAUKEE, July 15, 2024 – AI generated content of political candidates is a big concern for voters this election cycle.\n",
      "That was the message of Microsoft Tech Advisors Ashley O’Rourke and Ginny Badanes during a Monday bipartisan presentation on AI disinformation at the GOP Convention here until Thursday.\n",
      "Citing a survey conducted by Defending Digital Campaigns and the computer software company Yubico, O’Rourke said that 78% of voters are concerned about AI generated content being used to impersonate a political candidate.\n",
      "“It’s causing [voters] to distrust potentially authentic content and authentic communications,” O’Rourke said.\n",
      "The two representatives gave a bipartisan presentation hosted by the non-profit advocacy group All In Together.\n",
      "Badanes cautioned that if not properly addressed, the potential impact of AI deepfakes on the upcoming presidential election could be dev\n",
      "NEWS 2 SOURCE: https://tinyurl.com/25moxex3\n",
      "\n",
      "NEWS 3 TITLE: AI, Ethics, and the Law: How to Stay Aboveboard - MarketingProfs.com\n",
      "NEWS 3 CONTENT: Listen NEW! Listen to article\n",
      "What do you get when you mix AI, marketing, and ethics?\n",
      "Confusion. Frustration. Potentially questionable decisions. And a whole lot of hands in the air (and not because we \"just don't care\").\n",
      "We asked Arizona attorney Ruth Carter, Esq., Evil Genius at Geek Law Firm, for a few pointers to help navigate the new AI reality during their recent MarketingProfs presentation, \"Ethical AI in Marketing: Marketers' Guide to Privacy, Policy, and Regulatory Compliance.\"\n",
      "Keep in mind this is general legal information, not legal advice. Talk to your legal team or hire an Internet attorney if you need specific advice. If you're not paying Ruth, then Ruth is not your attorney.\n",
      "Read the fine print\n",
      "\"Understand what [the AI vendors] might do with what you...put into the 'AI machine' or what the 'AI machine' creates for you,\" Ruth emphasizes.\n",
      "Yes, that means the entire Terms and Conditions for each AI platform.\n",
      "In particular, scrutinize their policies on retaining and training\n",
      "NEWS 3 SOURCE: https://tinyurl.com/2b2ejvcb\n",
      "\n",
      "NEWS 4 TITLE: Science Writer: No Way To Tell If AI Is Conscious - Walter Bradley Center for Natural and Artificial Intelligence\n",
      "NEWS 4 CONTENT: Addressing the extravagant claims about “conscious AI” just around the corner, science writer Lindsey Laughlin offers a cautionary note at Ars Technica: We don’t really know what consciousness even is, in humans:\n",
      "There’s only one way we can know: by empirically identifying how consciousness works in organic lifeforms and developing a method by which we can consistently recognize it. We need to understand consciousness in ourselves before we have any hope of recognizing its presence in artificial systems. So before we dive deep into the complex consequences of sentient silicon and envision a future filled with conscious computers, we must resolve an ancient question: What is consciousness, and who has it? Lindsey Laughlin, “Could AIs become conscious? Right now, we have no way to tell,” Ars Technica, July 10, 2024\n",
      "There are, as she notes, many theories of consciousnes\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mtweet_writer_agent\u001b[0m (to User):\n",
      "\n",
      "\u001b[32m***** Suggested tool call (call_e976): write_tweet_tool *****\u001b[0m\n",
      "Arguments: \n",
      "{\"source_list\":[\"\\u003chttps://tinyurl.com/2xn3zo2u\\u003e\",\"\\u003chttps://tinyurl.com/25moxex3\\u003e\",\"\\u003chttps://tinyurl.com/2b2ejvcb\\u003e\",\"\\u003chttps://www.wnycstudios.org/podcasts/the-future-of-everything/episodes/conscious-ai\\u003e\"],\"tweet_list\":[\"New task force aims to use AI to boost Rhode Island's income by 2030! #ArtificialIntelligence #RhodeIsland\",\"AI deepfakes causing concerns for voters in the 2024 election. Stay informed! #AI #Deepfakes #Elections\",\"Marketing with AI? Make ethical decisions! Learn how from Ruth Carter, Esq. #AI #Marketing #Ethics\",\"Can AI be conscious? Lindsey Laughlin offers a cautionary note. #AI #Consciousness #Science\"]}\n",
      "\u001b[32m*************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION write_tweet_tool...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/15/2024 09:13:28 PM - HTTP Request: POST https://twitter.com/i/api/graphql/SiM_cAu83R0wnrpmKQQSEw/CreateTweet \"HTTP/1.1 200 OK\"\n",
      "07/15/2024 09:13:29 PM - HTTP Request: POST https://twitter.com/i/api/graphql/SiM_cAu83R0wnrpmKQQSEw/CreateTweet \"HTTP/1.1 200 OK\"\n",
      "07/15/2024 09:13:31 PM - HTTP Request: POST https://twitter.com/i/api/graphql/SiM_cAu83R0wnrpmKQQSEw/CreateTweet \"HTTP/1.1 200 OK\"\n",
      "07/15/2024 09:13:32 PM - HTTP Request: POST https://twitter.com/i/api/graphql/SiM_cAu83R0wnrpmKQQSEw/CreateTweet \"HTTP/1.1 200 OK\"\n",
      "07/15/2024 09:13:33 PM - HTTP Request: POST https://twitter.com/i/api/graphql/SiM_cAu83R0wnrpmKQQSEw/CreateTweet \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mUser\u001b[0m (to tweet_writer_agent):\n",
      "\n",
      "\u001b[33mUser\u001b[0m (to tweet_writer_agent):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_e976) *****\u001b[0m\n",
      "\n",
      "Tweet: These are the AI news reported within the last 1 hour:\n",
      "09:00PM EST, July 15, 2024\n",
      "Length: 81\n",
      "                      \n",
      "                      \n",
      "Tweet: New task force aims to use AI to boost Rhode Island's income by 2030! #ArtificialIntelligence #RhodeIsland\n",
      "<https://tinyurl.com/2xn3zo2u>\n",
      "Length: 137\n",
      "                      \n",
      "                      \n",
      "Tweet: AI deepfakes causing concerns for voters in the 2024 election. Stay informed! #AI #Deepfakes #Elections\n",
      "<https://tinyurl.com/25moxex3>\n",
      "Length: 134\n",
      "                      \n",
      "                      \n",
      "Tweet: Marketing with AI? Make ethical decisions! Learn how from Ruth Carter, Esq. #AI #Marketing #Ethics\n",
      "<https://tinyurl.com/2b2ejvcb>\n",
      "Length: 129\n",
      "                      \n",
      "                      \n",
      "Tweet: Can AI be conscious? Lindsey Laughlin offers a cautionary note. #AI #Consciousness #Science\n",
      "Length: 91\n",
      "                      \n",
      "                      \n",
      "\u001b[32m**************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    user_proxy_agent.initiate_chats([\n",
    "            # {\n",
    "            #     \"recipient\": topic_selector_agent,\n",
    "            #     \"message\": f\"Generate a list of {topic_count} topics related to the topic '{keyword}' and return a random topic from the list.\",\n",
    "            #     \"clear_history\": True,\n",
    "            #     \"silent\": False,\n",
    "            #     \"summary_method\": \"last_msg\"\n",
    "            # },\n",
    "            {\n",
    "                \"recipient\": news_collector_agent,\n",
    "                \"message\": f\"Collect {article_count} news articles about the topic '{keyword}' from the internet.\",\n",
    "                \"clear_history\": True,\n",
    "                \"silent\": False,\n",
    "                \"summary_method\": \"last_msg\"\n",
    "            },\n",
    "            {\n",
    "                \"recipient\": tweet_writer_agent,\n",
    "                \"message\": \"Write and post a twitter thread about the given list of news articles: \",\n",
    "                \"clear_history\": True,\n",
    "                \"silent\": False,\n",
    "                \"summary_method\": \"last_msg\"\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Global Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tweetbot-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from autogen import UserProxyAgent, AssistantAgent\n",
    "from newspaper.google_news import GoogleNewsSource\n",
    "from twikit import Client\n",
    "import os\n",
    "\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "KEYWORD = os.getenv(\"KEYWORD\")\n",
    "ARTICLE_COUNT = os.getenv(\"ARTICLE_COUNT\")\n",
    "KEYWORD_COUNT = os.getenv(\"KEYWORD_COUNT\")\n",
    "NEWS_COUNTRY = os.getenv(\"NEWS_COUNTRY\")\n",
    "GROQ_MODEL_NAME = os.getenv(\"GROQ_MODEL_NAME\")\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "GROQ_API_BASE = os.getenv(\"GROQ_API_BASE\")\n",
    "RELEASE = os.getenv(\"RELEASE\")\n",
    "AUTO_GENERATE_KEYWORDS = os.getenv(\"AUTO_GENERATE_KEYWORDS\")\n",
    "VERSION = os.getenv(\"VERSION\")\n",
    "\n",
    "if(RELEASE == \"PROD\"):\n",
    "    USERNAME = os.getenv(\"XUSERNAME\")\n",
    "    EMAIL = os.getenv(\"XEMAIL\")\n",
    "    PASSWORD = os.getenv(\"XPASSWORD\")\n",
    "else:\n",
    "    USERNAME = os.getenv(\"XUSERNAME_TEST\")\n",
    "    EMAIL = os.getenv(\"XEMAIL_TEST\")\n",
    "    PASSWORD = os.getenv(\"XPASSWORD_TEST\")\n",
    "\n",
    "#print all the above environment variables\n",
    "# for key in ['KEYWORD', 'ARTICLE_COUNT', 'KEYWORD_COUNT', 'NEWS_COUNTRY', 'AUTO_GENERATE_KEYWORDS',\n",
    "#             'GROQ_MODEL_NAME', 'GROQ_API_BASE', 'RELEASE', 'VERSION',]:\n",
    "#     print(f\"{key} = {os.environ[key]}\")\n",
    "\n",
    "# create cache directory\n",
    "cache = '../.cache'\n",
    "if not os.path.exists(cache):\n",
    "    os.makedirs(cache)\n",
    "topics_file = f'{cache}/topics.csv'\n",
    "urls_file = f'{cache}/urls.csv'\n",
    "\n",
    "# Config dictionary\n",
    "llm_config = {\n",
    "    \"cache_seed\": 42,\n",
    "    \"config_list\": [{\n",
    "        \"model\": GROQ_MODEL_NAME,\n",
    "        \"api_key\": GROQ_API_KEY,\n",
    "        \"base_url\": GROQ_API_BASE\n",
    "    }],\n",
    "}\n",
    "\n",
    "source = GoogleNewsSource(\n",
    "    period=\"1h\",\n",
    "    max_results = int(ARTICLE_COUNT), # number of responses for one topic\n",
    "    language = 'en',  # News in a specific language\n",
    "    country = NEWS_COUNTRY,  # News from a specific country\n",
    ")\n",
    "\n",
    "# # Initialize client\n",
    "if RELEASE != 'DEV' and 'x_client' not in globals():\n",
    "    x_client = Client('en-US')\n",
    "\n",
    "    x_client.login(\n",
    "        auth_info_1=USERNAME,\n",
    "        auth_info_2=EMAIL,\n",
    "        password=PASSWORD\n",
    "    )\n",
    "    print(\"Client initialized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyshorteners import Shortener\n",
    "from newspaper import Article\n",
    "from typing import Annotated\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def deduplicate_news_list(urls, keywords):\n",
    "    # deduplicate the urls and keywords based on urls\n",
    "    urls_dict = {}\n",
    "    for i in range(len(urls)):\n",
    "        if urls[i] not in urls_dict:\n",
    "            urls_dict[urls[i]] = keywords[i]\n",
    "        else:\n",
    "            urls_dict[urls[i]] = urls_dict[urls[i]] + ', ' + keywords[i]\n",
    "\n",
    "    urls = list(urls_dict.keys())\n",
    "    keywords = list(urls_dict.values())\n",
    "    return urls, keywords\n",
    "\n",
    "def read_news_articles_tool(urls, keywords):\n",
    "    if not os.path.isfile(urls_file):\n",
    "        df_urls = pd.DataFrame(columns=['urls', 'status'])  # Define the variable with a default value\n",
    "        df_urls.to_csv(urls_file)\n",
    "    else:\n",
    "        df_urls = pd.read_csv(urls_file, index_col='Unnamed: 0')\n",
    "    \n",
    "    urls = [url for url in urls if url not in df_urls['urls'].values]\n",
    "    \n",
    "    #deduplicate the urls and keywords based on urls\n",
    "    urls, keywords = deduplicate_news_list(urls, keywords)\n",
    "\n",
    "    if len(urls) == 0:\n",
    "        return []\n",
    "\n",
    "    article_list = []\n",
    "    for i in range(len(urls)):\n",
    "        try:\n",
    "            \n",
    "            article = Article(urls[i])\n",
    "            article.download()\n",
    "            article.parse()\n",
    "\n",
    "            if article.text: # and len(article.text.strip().split('\\n')) > 1:\n",
    "                # Append the URL and status to the DataFrame\n",
    "                df_urls = pd.concat([pd.DataFrame([[urls[i], 'success']], columns=df_urls.columns), df_urls], ignore_index=True)\n",
    "                df_urls.to_csv(urls_file)\n",
    "                article.keyword = keywords[i]\n",
    "                article_list.append(article)\n",
    "                continue\n",
    "            else:\n",
    "                df_urls = pd.concat([pd.DataFrame([[urls[i], 'empty']], columns=df_urls.columns), df_urls], ignore_index=True)\n",
    "                df_urls.to_csv(urls_file)\n",
    "                continue\n",
    "        except Exception as e:\n",
    "            df_urls = pd.concat([pd.DataFrame([[urls[i], 'error']], columns=df_urls.columns), df_urls], ignore_index=True)\n",
    "            df_urls.to_csv(urls_file)\n",
    "            print(f\"Error reading article: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    return article_list\n",
    "\n",
    "\n",
    "def get_news_articles_tool(keyword_list: Annotated[list, \"The list of keywords\"], count: Annotated[int, \"The number of news articles to collect from the internet\"]) -> str:\n",
    "    s = Shortener(timeout=5)\n",
    "    \n",
    "    urls, keywords = [], []\n",
    "    for keyword in keyword_list:\n",
    "        print(f\"FETCHING NEWS ON TOPIC: {keyword}\")\n",
    "        # raw_news = google_news.get_news(topic)\n",
    "        source.build(keyword = keyword, topic = 'TECHNOLOGY', top_news=False)\n",
    "        urls = urls + source.article_urls()\n",
    "        keywords = keywords + [keyword] * len(source.article_urls())\n",
    "    print(f\"URLS: {urls}\")\n",
    "        \n",
    "    if len(urls) == 0:\n",
    "        return None\n",
    "    \n",
    "    article_list = read_news_articles_tool(urls, keywords)\n",
    "    print(f\"Articles read: {len(article_list)}\")\n",
    "\n",
    "    result = ''\n",
    "    if len(article_list) > 0:\n",
    "        # Get the short urls\n",
    "        for article in article_list:\n",
    "            try:\n",
    "                article.short_url = s.tinyurl.short(article.url)\n",
    "            except Exception as e:\n",
    "                print(f\"Error shortening url: {str(e)}\")\n",
    "                article.short_url = article.url\n",
    "\n",
    "        for i in range(len(article_list)):\n",
    "            # do this until the result length is less than 30000\n",
    "            if len(result) > 30000:\n",
    "                break\n",
    "            result += (\"\"\"NEWS {n} TOPIC: {keyword}\n",
    "NEWS {n} TITLE: {title}\n",
    "NEWS {n} CONTENT: {content}\n",
    "NEWS {n} SOURCE: {url}\n",
    "\n",
    "\"\"\"\n",
    "        ).format(\n",
    "            keyword = article_list[i].keyword,\n",
    "            n = i+1,\n",
    "            title=article_list[i].title,\n",
    "            content=article_list[i].text.replace('\\n\\n', '\\n')[:1000],\n",
    "            url=article_list[i].short_url\n",
    "        )\n",
    "\n",
    "    return result[:4500]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "\n",
    "def merge_tweets(tweet_list: Annotated[list, \"The list of tweets to merge\"]) -> None:\n",
    "    merged_tweets = []\n",
    "    current_tweet = \"\"\n",
    "\n",
    "    for tweet in tweet_list:\n",
    "        if len(current_tweet) + len(tweet) <= 278:\n",
    "            current_tweet += f'{tweet}\\n\\n'\n",
    "        else:\n",
    "            merged_tweets.append(current_tweet)\n",
    "            current_tweet = tweet\n",
    "    if current_tweet:\n",
    "        merged_tweets.append(current_tweet)\n",
    "    \n",
    "    return merged_tweets\n",
    "\n",
    "def add_source_urls(tweet_list: Annotated[list, \"The list of tweets to post\"], source_list: Annotated[list, \"The list of 'https://tinyurl.com/' source URLs for each tweet\"]) -> list:\n",
    "    if len(tweet_list) == len(source_list):\n",
    "        del_index = []\n",
    "        for i in range(len(source_list)):\n",
    "            if not re.search(r'https://tinyurl\\.com/[a-zA-Z0-9]{8}', tweet_list[i]): # if the tweet does not contain a source\n",
    "                if re.search(r'https://tinyurl\\.com/[a-zA-Z0-9]{8}', source_list[i]): # if we have a source seperately\n",
    "                    tweet_list[i] = tweet_list[i][:278-len(source_list[i])]\n",
    "                    tweet_list[i] += f\"\\n{source_list[i]}\"\n",
    "    # uncomment below five lines to don't post a news if it doesn't have a source\n",
    "        #         else:\n",
    "        #             del_index.append(i)\n",
    "        # for i in sorted(del_index, reverse=True):\n",
    "        #     del tweet_list[i]\n",
    "        #     del source_list[i]\n",
    "        \n",
    "    return tweet_list\n",
    "\n",
    "def get_intro_tweet() -> str:\n",
    "    now = datetime.now(pytz.utc)\n",
    "    eastern = pytz.timezone('America/New_York')\n",
    "    now_eastern = now.astimezone(eastern)\n",
    "\n",
    "    last_hour = now_eastern.replace(minute=0, second=0, microsecond=0)\n",
    "\n",
    "    formatted_datetime = last_hour.strftime(\"%I:00%p EST, %B %d, %Y\")\n",
    "\n",
    "    return f\"\"\"These are the AI news within the last 1 hour:\n",
    "{formatted_datetime}\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def write_tweet_tool(tweet_list: Annotated[list, \"The list of tweets to post\"], source_list: Annotated[list, \"The list of 'https://tinyurl.com/' source URL for each tweet\"]) -> str:\n",
    "    tweet_list = add_source_urls(tweet_list, source_list)\n",
    "    # tweet_list = merge_tweets(tweet_list)\n",
    "    # tweet_list = [get_intro_tweet()] + tweet_list\n",
    "\n",
    "    posts = ''\n",
    "    # for tweet, source in zip(tweet_list, source_list):\n",
    "    for i in range(len(tweet_list)):\n",
    "    #     if 'https://tinyurl.com/' in tweet_list[i]:\n",
    "    #         if len(tweet_list[i]) > 280:\n",
    "    #             tweet_list[i] = tweet_list[i][:276-len(source)] + '...' + f\"\\n{source}\"\n",
    "    #     else:\n",
    "    #         if len(tweet_list[i]) <= 279-len(source):\n",
    "    #             tweet_list[i] += f\"\\n{source}\"\n",
    "    #         else:\n",
    "    #             tweet_list[i] = tweet_list[i][:276-len(source)] + '...' + f\"\\n{source}\"\n",
    "        try:\n",
    "            # final tweet length check for redundancy \n",
    "            if len(tweet_list[i]) > 280:\n",
    "                tweet_list[i] = tweet_list[i][:276] + '...'\n",
    "\n",
    "            if RELEASE != \"DEV\":\n",
    "                if i==0:\n",
    "                    last_tweet = x_client.create_tweet(\n",
    "                        text=tweet_list[i],\n",
    "                    )\n",
    "                else:\n",
    "                    last_tweet = x_client.create_tweet(\n",
    "                        text=tweet_list[i],\n",
    "                        reply_to=last_tweet.id\n",
    "                    )\n",
    "            \n",
    "            posts += (\"\"\"\n",
    "Tweet: {tweet}\n",
    "Length: {length}\n",
    "                      \n",
    "                      \"\"\").format(tweet=tweet_list[i], length=len(tweet_list[i]))\n",
    "            \n",
    "            time.sleep(1)\n",
    "        except Exception as e:\n",
    "            error_message = f\"Failed to post tweet: {str(e)}\"\n",
    "            print(error_message)\n",
    "            continue\n",
    "\n",
    "    if(posts == ''):\n",
    "        posts = \"No tweets posted\"\n",
    "        \n",
    "    return posts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_collector_agent = AssistantAgent(\n",
    "    \"news_collector_agent\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=f\"\"\"You are good at collecting recent news articles about a given keyword on the internet. \n",
    "    You should generate a list of {KEYWORD_COUNT} topics closely related to the given keyword. \n",
    "    Use the provided tool to collect news about the generated list of topics.\"\"\",\n",
    "    max_consecutive_auto_reply=1\n",
    ")\n",
    "\n",
    "tweet_writer_agent = AssistantAgent(\n",
    "    \"tweet_writer_agent\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=f\"\"\"You are an autonomous twitter bot that's created to educate the people about {KEYWORD}. \n",
    "    You are good at posting a series of twitter posts on the given list of news by summarizing each news as one short tweet. \n",
    "    You MUST only strictly post news that is about the topic {KEYWORD} or the respective news topic given and ignore other news(double check this). \n",
    "    Always use simple words. \n",
    "    Use the provided tool to post all the tweets as a thread(list of tweets).\"\"\",\n",
    "    max_consecutive_auto_reply=1\n",
    ")\n",
    "\n",
    "\n",
    "user_proxy_agent = UserProxyAgent(\n",
    "    name=\"User\",\n",
    "    system_message=\"You are a helpful AI assistant. Return 'TERMINATE' when the task is done.\",\n",
    "    is_termination_msg=lambda msg: msg.get(\"content\") is not None and \"TERMINATE\" in msg[\"content\"],\n",
    "    human_input_mode=\"NEVER\",\n",
    "    code_execution_config=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.write_tweet_tool(tweet_list: Annotated[list, 'The list of tweets to post'], source_list: Annotated[list, \"The list of 'https://tinyurl.com/' source URL for each tweet\"]) -> str>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Register the tool signature with the assistant agent.\n",
    "news_collector_agent.register_for_llm(name=\"get_news_articles_tool\", description=\"Collect news articles about a list of topics on the internet.\")(get_news_articles_tool)\n",
    "tweet_writer_agent.register_for_llm(name=\"write_tweet_tool\", description=\"Write a twitter thread.\")(write_tweet_tool)\n",
    "\n",
    "# Register the tool function with the user proxy agent.\n",
    "user_proxy_agent.register_for_execution(name=\"get_news_articles_tool\")(get_news_articles_tool)\n",
    "user_proxy_agent.register_for_execution(name=\"write_tweet_tool\")(write_tweet_tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[34mStarting a new chat....\u001b[0m\n",
      "\u001b[34m\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[33mUser\u001b[0m (to news_collector_agent):\n",
      "\n",
      "Collect 5 news articles about the topic 'Artificial Intelligence' from the internet.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mnews_collector_agent\u001b[0m (to User):\n",
      "\n",
      "\u001b[32m***** Suggested tool call (call_d4c6): get_news_articles_tool *****\u001b[0m\n",
      "Arguments: \n",
      "{\"count\":5,\"keyword_list\":[\"Artificial Intelligence\",\"AI technology\",\"Machine Learning\",\"Neural Networks\",\"Deep Learning\"]}\n",
      "\u001b[32m*******************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION get_news_articles_tool...\u001b[0m\n",
      "FETCHING NEWS ON TOPIC: Artificial Intelligence\n",
      "FETCHING NEWS ON TOPIC: AI technology\n",
      "FETCHING NEWS ON TOPIC: Machine Learning\n",
      "FETCHING NEWS ON TOPIC: Neural Networks\n",
      "FETCHING NEWS ON TOPIC: Deep Learning\n",
      "URLS: ['https://www.theguardian.com/technology/artificialintelligenceai/2024/jul/18/all', 'https://www.cio.com/article/2517656/adapting-to-ai-regulations-in-the-u-s-and-europe-impacts-on-cios-and-global-enterprises.html', 'https://thehackernews.com/2024/07/meta-halts-ai-use-in-brazil-following.html', 'https://techcrunch.com/2024/07/17/ttt-models-might-be-the-next-frontier-in-generative-ai/', 'https://www.theverge.com/24196835/amazon-prime-day-cheap-tech-best-deals-under-100', 'https://www.androidauthority.com/samsung-galaxy-satellite-3461852/', 'https://www.cio.com/article/2517656/adapting-to-ai-regulations-in-the-u-s-and-europe-impacts-on-cios-and-global-enterprises.html', 'https://aiin.healthcare/newsletter/2024-07-18/ai-regulation-workshop-ai-reporters-notebook-partner-news', 'https://www.insideradio.com/free/veritone-more-positions-higher-salaries-make-ai-a-driving-force-in-the-job-market/article_00279a40-44cd-11ef-8c89-2fe7c4b22486.html', 'https://techcrunch.com/2024/07/17/ttt-models-might-be-the-next-frontier-in-generative-ai/', 'https://www.theverge.com/24196835/amazon-prime-day-cheap-tech-best-deals-under-100', 'https://www.androidauthority.com/samsung-galaxy-satellite-3461852/', 'https://aiin.healthcare/newsletter/2024-07-18/ai-regulation-workshop-ai-reporters-notebook-partner-news', 'https://studyinternational.com/news/how-to-become-an-effective-business-data-scientist-for-a-data-driven-world/', 'https://www.marktechpost.com/2024/07/17/g-retriever-advancing-real-world-graph-question-answering-with-rag-and-llms/', 'https://techcrunch.com/2024/07/17/ttt-models-might-be-the-next-frontier-in-generative-ai/', 'https://www.theverge.com/24196835/amazon-prime-day-cheap-tech-best-deals-under-100', 'https://www.androidauthority.com/samsung-galaxy-satellite-3461852/', 'https://www.marktechpost.com/2024/07/17/g-retriever-advancing-real-world-graph-question-answering-with-rag-and-llms/', 'https://www.globenewswire.com/news-release/2024/07/18/2915035/0/en/Introducing-Three-Protocol-Building-An-Alternative-To-Centralized-Digital-ID-s-and-KYC-with-No-KYC-Zero-Knowledge-Proof-Decentralized-Digital-ID-s.html', 'https://www.theindianmoviechannel.com/media/text-to-image-generation-market-see-huge-growth-for-new-normal-openai-fotor-mid-journey/363868/', 'https://techcrunch.com/2024/07/17/ttt-models-might-be-the-next-frontier-in-generative-ai/', 'https://www.theverge.com/24196835/amazon-prime-day-cheap-tech-best-deals-under-100', 'https://www.androidauthority.com/samsung-galaxy-satellite-3461852/', 'https://www.marktechpost.com/2024/07/17/g-retriever-advancing-real-world-graph-question-answering-with-rag-and-llms/', 'https://www.vanguardngr.com/2024/07/nnpc-science-quiz-champion-tony-okoke-bags-double-fist-class-in-usa/', 'https://www.thinkdigitalpartners.com/news/2024/07/18/from-feedback-to-forecast-at-the-met-office/', 'https://techcrunch.com/2024/07/17/ttt-models-might-be-the-next-frontier-in-generative-ai/', 'https://www.theverge.com/24196835/amazon-prime-day-cheap-tech-best-deals-under-100', 'https://www.androidauthority.com/samsung-galaxy-satellite-3461852/']\n",
      "Error reading article: Article `download()` failed with HTTPSConnectionPool(host='www.theindianmoviechannel.com', port=443): Read timed out. (read timeout=7) on URL https://www.theindianmoviechannel.com/media/text-to-image-generation-market-see-huge-growth-for-new-normal-openai-fotor-mid-journey/363868/\n",
      "Articles read: 1\n",
      "\u001b[33mUser\u001b[0m (to news_collector_agent):\n",
      "\n",
      "\u001b[33mUser\u001b[0m (to news_collector_agent):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_d4c6) *****\u001b[0m\n",
      "NEWS 1 TOPIC: Artificial Intelligence, Artificial Intelligence\n",
      "NEWS 1 TITLE: AI regulation workshop\n",
      "NEWS 1 CONTENT: Buzzworthy developments of the past few days.\n",
      "Mayo Clinic leads the league in AI readiness. Ranking private-sector health systems by their progress in AI innovation and execution, CB Insights places the Minnesota-based high achiever ahead of runners-up Intermountain Health, Cleveland Clinic, Kaiser Permanente and 20 others. The depth of Mayo’s innovative character shows in its patent activity, CB Insights remarks, noting that Mayo has filed more than 50 patents in numerous clinical areas. “It has also invested in AI-enabled companies addressing a range of use cases in healthcare, from clinical documentation to surgical intelligence,” the investment intel company points out.\n",
      "UnitedHealth Group buffers cyberattack pain with AI promise. The company is still paying out mountainous sums for the malicious breach on its Change Healthcare unit. But it’s betting its commitment to technological innovation will drive business growth over the next few years. “Our growing AI portfolio made up of pr\n",
      "NEWS 1 SOURCE: https://tinyurl.com/29epy6kp\n",
      "\n",
      "\n",
      "\u001b[32m**************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[34m\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[34mStarting a new chat....\u001b[0m\n",
      "\u001b[34m\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[33mUser\u001b[0m (to tweet_writer_agent):\n",
      "\n",
      "Write and post a twitter thread about the given list of news articles:\n",
      "\n",
      "Context: \n",
      "NEWS 1 TOPIC: Artificial Intelligence, Artificial Intelligence\n",
      "NEWS 1 TITLE: AI regulation workshop\n",
      "NEWS 1 CONTENT: Buzzworthy developments of the past few days.\n",
      "Mayo Clinic leads the league in AI readiness. Ranking private-sector health systems by their progress in AI innovation and execution, CB Insights places the Minnesota-based high achiever ahead of runners-up Intermountain Health, Cleveland Clinic, Kaiser Permanente and 20 others. The depth of Mayo’s innovative character shows in its patent activity, CB Insights remarks, noting that Mayo has filed more than 50 patents in numerous clinical areas. “It has also invested in AI-enabled companies addressing a range of use cases in healthcare, from clinical documentation to surgical intelligence,” the investment intel company points out.\n",
      "UnitedHealth Group buffers cyberattack pain with AI promise. The company is still paying out mountainous sums for the malicious breach on its Change Healthcare unit. But it’s betting its commitment to technological innovation will drive business growth over the next few years. “Our growing AI portfolio made up of pr\n",
      "NEWS 1 SOURCE: https://tinyurl.com/29epy6kp\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/18/2024 02:44:41 AM - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mtweet_writer_agent\u001b[0m (to User):\n",
      "\n",
      "\u001b[32m***** Suggested tool call (call_n5pf): write_tweet_tool *****\u001b[0m\n",
      "Arguments: \n",
      "{\"tweet_list\":[\"Mayo Clinic leads in AI readiness, filing over 50 patents in various clinical areas. #AI #Healthcare\",\"UnitedHealth Group aims to drive business growth with AI, despite ongoing costs from a recent cyberattack. #AI #Cybersecurity\"],\"source_list\":[\"https://tinyurl.com/29epy6kp\"]}\n",
      "\u001b[32m*************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION write_tweet_tool...\u001b[0m\n",
      "\u001b[33mUser\u001b[0m (to tweet_writer_agent):\n",
      "\n",
      "\u001b[33mUser\u001b[0m (to tweet_writer_agent):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_n5pf) *****\u001b[0m\n",
      "\n",
      "Tweet: Mayo Clinic leads in AI readiness, filing over 50 patents in various clinical areas. #AI #Healthcare\n",
      "Length: 100\n",
      "                      \n",
      "                      \n",
      "Tweet: UnitedHealth Group aims to drive business growth with AI, despite ongoing costs from a recent cyberattack. #AI #Cybersecurity\n",
      "Length: 125\n",
      "                      \n",
      "                      \n",
      "\u001b[32m**************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "try:\n",
    "    if AUTO_GENERATE_KEYWORDS=='True':\n",
    "        user_proxy_agent.initiate_chats([\n",
    "            {\n",
    "                \"recipient\": news_collector_agent,\n",
    "                \"message\": f\"Collect {KEYWORD_COUNT} news articles about the topic '{KEYWORD}' from the internet.\",\n",
    "                \"clear_history\": True,\n",
    "                \"silent\": False,\n",
    "                \"summary_method\": \"last_msg\"\n",
    "            },\n",
    "            {\n",
    "                \"recipient\": tweet_writer_agent,\n",
    "                \"message\": \"Write and post a twitter thread about the given list of news articles:\\n\",\n",
    "                \"clear_history\": True,\n",
    "                \"silent\": False,\n",
    "                \"summary_method\": \"last_msg\"\n",
    "            }\n",
    "        ])\n",
    "    else:\n",
    "        topics_list = pd.read_csv('../topics.csv')\n",
    "        topics_list = topics_list.values.tolist()\n",
    "        topics_list = [item for sublist in topics_list for item in sublist]\n",
    "        topics_list = [x for x in topics_list if str(x) != 'nan']\n",
    "        random.shuffle(topics_list)\n",
    "        topics_list = topics_list[:5] # TESTING\n",
    "\n",
    "        if len(topics_list)==0:\n",
    "            raise Exception(\"No topics found\")\n",
    "\n",
    "        news_articles = get_news_articles_tool(keyword_list=topics_list, count=int(ARTICLE_COUNT))\n",
    "        if news_articles:\n",
    "            user_proxy_agent.initiate_chats([\n",
    "                {\n",
    "                    \"recipient\": tweet_writer_agent,\n",
    "                    \"message\": f\"Write and post a twitter thread about the given list of news articles:\\n{news_articles}\",\n",
    "                    \"clear_history\": True,\n",
    "                    \"silent\": False,\n",
    "                    \"summary_method\": \"last_msg\"\n",
    "                }\n",
    "            ])\n",
    "        else:\n",
    "            raise Exception(\"No news articles found\")\n",
    "except Exception as e:\n",
    "    print(f\"Global Error: {str(e)}\")\n",
    "    #print trackback\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tweetbot-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

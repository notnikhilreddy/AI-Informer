{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD = Artificial Intelligence\n",
      "ARTICLE_COUNT = 1\n",
      "KEYWORD_COUNT = 10\n",
      "NEWS_COUNTRY = United States\n",
      "AUTO_GENERATE_KEYWORDS = False\n",
      "GROQ_MODEL_NAME = mixtral-8x7b-32768\n",
      "GROQ_API_BASE = https://api.groq.com/openai/v1\n",
      "RELEASE = DEV\n",
      "VERSION = 2\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from autogen import UserProxyAgent, AssistantAgent\n",
    "from twikit import Client\n",
    "import os\n",
    "\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "KEYWORD = os.getenv(\"KEYWORD\")\n",
    "ARTICLE_COUNT = os.getenv(\"ARTICLE_COUNT\")\n",
    "KEYWORD_COUNT = os.getenv(\"KEYWORD_COUNT\")\n",
    "NEWS_COUNTRY = os.getenv(\"NEWS_COUNTRY\")\n",
    "GROQ_MODEL_NAME = os.getenv(\"GROQ_MODEL_NAME\")\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "GROQ_API_BASE = os.getenv(\"GROQ_API_BASE\")\n",
    "RELEASE = os.getenv(\"RELEASE\")\n",
    "AUTO_GENERATE_KEYWORDS = os.getenv(\"AUTO_GENERATE_KEYWORDS\")\n",
    "VERSION = os.getenv(\"VERSION\")\n",
    "\n",
    "if(RELEASE == \"PROD\"):\n",
    "    USERNAME = os.getenv(\"XUSERNAME\")\n",
    "    EMAIL = os.getenv(\"XEMAIL\")\n",
    "    PASSWORD = os.getenv(\"XPASSWORD\")\n",
    "else:\n",
    "    USERNAME = os.getenv(\"XUSERNAME_TEST\")\n",
    "    EMAIL = os.getenv(\"XEMAIL_TEST\")\n",
    "    PASSWORD = os.getenv(\"XPASSWORD_TEST\")\n",
    "\n",
    "#print all the above environment variables\n",
    "# for key in ['KEYWORD', 'ARTICLE_COUNT', 'KEYWORD_COUNT', 'NEWS_COUNTRY', 'AUTO_GENERATE_KEYWORDS',\n",
    "#             'GROQ_MODEL_NAME', 'GROQ_API_BASE', 'RELEASE', 'VERSION',]:\n",
    "#     print(f\"{key} = {os.environ[key]}\")\n",
    "\n",
    "# create cache directory\n",
    "cache = '../.cache'\n",
    "if not os.path.exists(cache):\n",
    "    os.makedirs(cache)\n",
    "topics_file = f'{cache}/topics.csv'\n",
    "urls_file = f'{cache}/urls.csv'\n",
    "\n",
    "# Config dictionary\n",
    "llm_config = {\n",
    "    \"cache_seed\": 42,\n",
    "    \"config_list\": [{\n",
    "        \"model\": GROQ_MODEL_NAME,\n",
    "        \"api_key\": GROQ_API_KEY,\n",
    "        \"base_url\": GROQ_API_BASE\n",
    "    }],\n",
    "}\n",
    "\n",
    "# # Initialize client\n",
    "if RELEASE != 'DEV' and 'x_client' not in globals():\n",
    "    x_client = Client('en-US')\n",
    "\n",
    "    x_client.login(\n",
    "        auth_info_1=USERNAME,\n",
    "        auth_info_2=EMAIL,\n",
    "        password=PASSWORD\n",
    "    )\n",
    "    print(\"Client initialized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gnews import GNews\n",
    "from pyshorteners import Shortener\n",
    "from newspaper import Article\n",
    "from typing import Annotated\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def read_news_articles_tool(news_list):\n",
    "    if not os.path.isfile(urls_file):\n",
    "        df_urls = pd.DataFrame(columns=['urls', 'status'])  # Define the variable with a default value\n",
    "        df_urls.to_csv(urls_file)\n",
    "    else:\n",
    "        df_urls = pd.read_csv(urls_file, index_col='Unnamed: 0')\n",
    "    \n",
    "    news_list = [news for news in news_list if news['url'] not in df_urls['urls'].values]\n",
    "    \n",
    "    #deduplicate the news_list based on urls\n",
    "    def deduplicate_news_list(news_list):\n",
    "        seen_urls = set()\n",
    "        unique_news_list = []\n",
    "        for news_item in news_list:\n",
    "            url = news_item.get('url')\n",
    "            if url not in seen_urls:\n",
    "                seen_urls.add(url)\n",
    "                unique_news_list.append(news_item)\n",
    "        return unique_news_list\n",
    "    news_list = deduplicate_news_list(news_list)\n",
    "\n",
    "    if len(news_list) == 0:\n",
    "        return [], []\n",
    "\n",
    "    article_list = []\n",
    "    final_news_list = []\n",
    "    for news in news_list:\n",
    "        try:\n",
    "            article = Article(news['url'])\n",
    "            article.download()\n",
    "            article.parse()\n",
    "\n",
    "            if article.text and len(article.text.strip().split('\\n')) > 1:\n",
    "                # Append the URL and status to the DataFrame\n",
    "                df_urls = pd.concat([pd.DataFrame([[news['url'], 'success']], columns=df_urls.columns), df_urls], ignore_index=True)\n",
    "                df_urls.to_csv(urls_file)\n",
    "                final_news_list.append(news)\n",
    "                article_list.append(article)\n",
    "                continue\n",
    "            else:\n",
    "                df_urls = pd.concat([pd.DataFrame([[news['url'], 'empty']], columns=df_urls.columns), df_urls], ignore_index=True)\n",
    "                df_urls.to_csv(urls_file)\n",
    "                continue\n",
    "        except Exception as e:\n",
    "            df_urls = pd.concat([pd.DataFrame([[news['url'], 'error']], columns=df_urls.columns), df_urls], ignore_index=True)\n",
    "            df_urls.to_csv(urls_file)\n",
    "            print(f\"Error selecting article: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    return final_news_list, article_list\n",
    "\n",
    "def get_news_articles_tool(topics_list: Annotated[list, \"The list of topics\"], count: Annotated[int, \"The number of news articles to collect from the internet\"]) -> str:\n",
    "    google_news = GNews()\n",
    "    # google_news.max_results = int(floor(count/len(topics_list))) # number of responses for one topic\n",
    "    google_news.max_results = count # number of responses for one topic\n",
    "    google_news.language = 'english'  # News in a specific language\n",
    "    google_news.country = NEWS_COUNTRY  # News from a specific country\n",
    "    google_news.period = '1h'  # Adjust period in hours\n",
    "\n",
    "    s = Shortener(timeout=5)\n",
    "    \n",
    "    raw_news_list = []\n",
    "    for topic in topics_list:\n",
    "        print(f\"FETCHING NEWS ON TOPIC: {topic}\")\n",
    "        raw_news = google_news.get_news(topic)\n",
    "        for news in raw_news:\n",
    "            news['keyword'] = topic\n",
    "        raw_news_list.extend(raw_news)\n",
    "        \n",
    "    if len(raw_news_list) == 0:\n",
    "        return None\n",
    "    \n",
    "    news_list, article_list = read_news_articles_tool(raw_news_list)\n",
    "    print(f\"ARTICLE LIST len: {len(article_list)}\")\n",
    "\n",
    "    result = ''\n",
    "    if len(news_list) > 0 and len(article_list) > 0:\n",
    "        # get the news['url'] for each news\n",
    "        news_url_list = [news['url'] for news in news_list]\n",
    "        short_urls = [s.tinyurl.short(url) for url in news_url_list]\n",
    "        article_text_list = [article.text for article in article_list]\n",
    "\n",
    "        for i in range(len(news_list)):\n",
    "            # do this until the result length is less than 30000\n",
    "            if len(result) > 30000:\n",
    "                break\n",
    "            result += (\"\"\"NEWS {n} TOPIC: {keyword}\n",
    "NEWS {n} TITLE: {title}\n",
    "NEWS {n} CONTENT: {content}\n",
    "NEWS {n} SOURCE: {url}\n",
    "\n",
    "\"\"\"\n",
    "        ).format(\n",
    "            keyword = news_list[i]['keyword'],\n",
    "            n = i+1,\n",
    "            title=news_list[i]['title'],\n",
    "            content=article_text_list[i].replace('\\n\\n', '\\n')[:1000],\n",
    "            url=short_urls[i]\n",
    "        )\n",
    "\n",
    "    return result[:4500]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "\n",
    "def merge_tweets(tweet_list: Annotated[list, \"The list of tweets to merge\"]) -> None:\n",
    "    merged_tweets = []\n",
    "    current_tweet = \"\"\n",
    "\n",
    "    for tweet in tweet_list:\n",
    "        if len(current_tweet) + len(tweet) <= 278:\n",
    "            current_tweet += f'{tweet}\\n\\n'\n",
    "        else:\n",
    "            merged_tweets.append(current_tweet)\n",
    "            current_tweet = tweet\n",
    "    if current_tweet:\n",
    "        merged_tweets.append(current_tweet)\n",
    "    \n",
    "    return merged_tweets\n",
    "\n",
    "def add_source_urls(tweet_list: Annotated[list, \"The list of tweets to post\"], source_list: Annotated[list, \"The list of 'https://tinyurl.com/' source URLs for each tweet\"]) -> list:\n",
    "    if len(tweet_list) == len(source_list):\n",
    "        del_index = []\n",
    "        for i in range(len(source_list)):\n",
    "            if not re.search(r'https://tinyurl\\.com/[a-zA-Z0-9]{8}', tweet_list[i]): # if the tweet does not contain a source\n",
    "                if re.search(r'https://tinyurl\\.com/[a-zA-Z0-9]{8}', source_list[i]): # if we have a source seperately\n",
    "                    tweet_list[i] = tweet_list[i][:278-len(source_list[i])]\n",
    "                    tweet_list[i] += f\"\\n{source_list[i]}\"\n",
    "    # uncomment below five lines to don't post a news if it doesn't have a source\n",
    "        #         else:\n",
    "        #             del_index.append(i)\n",
    "        # for i in sorted(del_index, reverse=True):\n",
    "        #     del tweet_list[i]\n",
    "        #     del source_list[i]\n",
    "        \n",
    "    return tweet_list\n",
    "\n",
    "def get_intro_tweet() -> str:\n",
    "    now = datetime.now(pytz.utc)\n",
    "    eastern = pytz.timezone('America/New_York')\n",
    "    now_eastern = now.astimezone(eastern)\n",
    "\n",
    "    last_hour = now_eastern.replace(minute=0, second=0, microsecond=0)\n",
    "\n",
    "    formatted_datetime = last_hour.strftime(\"%I:00%p EST, %B %d, %Y\")\n",
    "\n",
    "    return f\"\"\"These are the AI news within the last 1 hour:\n",
    "{formatted_datetime}\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def write_tweet_tool(tweet_list: Annotated[list, \"The list of tweets to post\"], source_list: Annotated[list, \"The list of 'https://tinyurl.com/' source URL for each tweet\"]) -> str:\n",
    "    tweet_list = add_source_urls(tweet_list, source_list)\n",
    "    tweet_list = merge_tweets(tweet_list)\n",
    "    # tweet_list = [get_intro_tweet()] + tweet_list\n",
    "\n",
    "    posts = ''\n",
    "    # for tweet, source in zip(tweet_list, source_list):\n",
    "    for i in range(len(tweet_list)):\n",
    "    #     if 'https://tinyurl.com/' in tweet_list[i]:\n",
    "    #         if len(tweet_list[i]) > 280:\n",
    "    #             tweet_list[i] = tweet_list[i][:276-len(source)] + '...' + f\"\\n{source}\"\n",
    "    #     else:\n",
    "    #         if len(tweet_list[i]) <= 279-len(source):\n",
    "    #             tweet_list[i] += f\"\\n{source}\"\n",
    "    #         else:\n",
    "    #             tweet_list[i] = tweet_list[i][:276-len(source)] + '...' + f\"\\n{source}\"\n",
    "        try:\n",
    "            # final tweet length check for redundancy \n",
    "            if len(tweet_list[i]) > 280:\n",
    "                tweet_list[i] = tweet_list[i][:276] + '...'\n",
    "\n",
    "            if RELEASE != \"DEV\":\n",
    "                if i==0:\n",
    "                    last_tweet = x_client.create_tweet(\n",
    "                        text=tweet_list[i],\n",
    "                    )\n",
    "                else:\n",
    "                    last_tweet = x_client.create_tweet(\n",
    "                        text=tweet_list[i],\n",
    "                        reply_to=last_tweet.id\n",
    "                    )\n",
    "            \n",
    "            posts += (\"\"\"\n",
    "Tweet: {tweet}\n",
    "Length: {length}\n",
    "                      \n",
    "                      \"\"\").format(tweet=tweet_list[i], length=len(tweet_list[i]))\n",
    "            \n",
    "            time.sleep(1)\n",
    "        except Exception as e:\n",
    "            error_message = f\"Failed to post tweet: {str(e)}\"\n",
    "            print(error_message)\n",
    "            continue\n",
    "\n",
    "    if(posts == ''):\n",
    "        posts = \"No tweets posted\"\n",
    "        \n",
    "    return posts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_collector_agent = AssistantAgent(\n",
    "    \"news_collector_agent\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=f\"\"\"You are good at collecting recent news articles about a given keyword on the internet. \n",
    "    You should generate a list of {KEYWORD_COUNT} topics closely related to the given keyword. \n",
    "    Use the provided tool to collect news about the generated list of topics.\"\"\",\n",
    "    max_consecutive_auto_reply=1\n",
    ")\n",
    "\n",
    "tweet_writer_agent = AssistantAgent(\n",
    "    \"tweet_writer_agent\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=f\"\"\"You are an autonomous twitter bot that's created to educate the people about {KEYWORD}. \n",
    "    You are good at posting a series of twitter posts on the given list of news by summarizing each news as one short tweet. \n",
    "    You MUST only strictly post news that is about the topic {KEYWORD} or the respective news topic given and ignore other news(double check this). \n",
    "    Always use simple words. \n",
    "    Use the provided tool to post all the tweets as a thread(list of tweets).\"\"\",\n",
    "    max_consecutive_auto_reply=1\n",
    ")\n",
    "\n",
    "\n",
    "user_proxy_agent = UserProxyAgent(\n",
    "    name=\"User\",\n",
    "    system_message=\"You are a helpful AI assistant. Return 'TERMINATE' when the task is done.\",\n",
    "    is_termination_msg=lambda msg: msg.get(\"content\") is not None and \"TERMINATE\" in msg[\"content\"],\n",
    "    human_input_mode=\"NEVER\",\n",
    "    code_execution_config=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.write_tweet_tool(tweet_list: Annotated[list, 'The list of tweets to post'], source_list: Annotated[list, \"The list of 'https://tinyurl.com/' source URL for each tweet\"]) -> str>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Register the tool signature with the assistant agent.\n",
    "news_collector_agent.register_for_llm(name=\"get_news_articles_tool\", description=\"Collect news articles about a list of topics on the internet.\")(get_news_articles_tool)\n",
    "tweet_writer_agent.register_for_llm(name=\"write_tweet_tool\", description=\"Write a twitter thread.\")(write_tweet_tool)\n",
    "\n",
    "# Register the tool function with the user proxy agent.\n",
    "user_proxy_agent.register_for_execution(name=\"get_news_articles_tool\")(get_news_articles_tool)\n",
    "user_proxy_agent.register_for_execution(name=\"write_tweet_tool\")(write_tweet_tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FETCHING NEWS ON TOPIC: AI\n",
      "FETCHING NEWS ON TOPIC: AI in Blockchain\n",
      "FETCHING NEWS ON TOPIC: RNNs\n",
      "FETCHING NEWS ON TOPIC: Transfer Learning\n",
      "FETCHING NEWS ON TOPIC: AI Ethics\n",
      "FETCHING NEWS ON TOPIC: AI in Transportation\n",
      "FETCHING NEWS ON TOPIC: AI Transparency\n",
      "FETCHING NEWS ON TOPIC: Explainable AI\n",
      "FETCHING NEWS ON TOPIC: Amazon AI\n",
      "FETCHING NEWS ON TOPIC: AI Bias\n",
      "FETCHING NEWS ON TOPIC: Transformer Models\n",
      "FETCHING NEWS ON TOPIC: AI in Edge Computing\n",
      "FETCHING NEWS ON TOPIC: AI in 5G\n",
      "FETCHING NEWS ON TOPIC: Supervised Learning\n",
      "FETCHING NEWS ON TOPIC: AI in Smart Cities\n",
      "FETCHING NEWS ON TOPIC: OpenCV\n",
      "FETCHING NEWS ON TOPIC: AI Expo\n",
      "FETCHING NEWS ON TOPIC: Scikit-learn\n",
      "FETCHING NEWS ON TOPIC: ICML\n",
      "FETCHING NEWS ON TOPIC: Unsupervised Learning\n",
      "Error selecting article: Article `download()` failed with 403 Client Error: Forbidden for url: https://itbrief.com.au/story/sensen-networks-reports-record-financial-results-for-fy24 on URL https://news.google.com/rss/articles/CBMiVmh0dHBzOi8vaXRicmllZi5jb20uYXUvc3Rvcnkvc2Vuc2VuLW5ldHdvcmtzLXJlcG9ydHMtcmVjb3JkLWZpbmFuY2lhbC1yZXN1bHRzLWZvci1meTI00gEA?oc=5&hl=en-US&gl=US&ceid=US:en\n",
      "ARTICLE LIST len: 6\n",
      "\u001b[34m\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[34mStarting a new chat....\u001b[0m\n",
      "\u001b[34m\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[33mUser\u001b[0m (to tweet_writer_agent):\n",
      "\n",
      "Write and post a twitter thread about the given list of news articles:\n",
      "KEYWORD: AI\n",
      "NEWS 1 TITLE: ConverseNow Acquires Valyant AI, Consolidating the Drive Towards Voice AI Drive-Thru Restaurant Technology - Restaurant Technology News\n",
      "NEWS 1 CONTENT: In a move indicative of the growing role of artificial intelligence in the restaurant industry, ConverseNow, a technology solution provider specializing in voice AI solutions for restaurants, has acquired Valyant AI, a provider of drive-thru automation technology. The acquisition aims to leverage the combined strengths of both companies to accelerate the adoption of AI-driven solutions within the quick-service restaurant (QSR) sector.\n",
      "Founded in 2018, Valyant AI has focused its efforts on developing and deploying voice AI technology specifically for the drive-thru environment. Its primary offering, an AI voice assistant named Holly, integrates with existing drive-thru communication systems from established manufacturers like PAR and HME. Furthermore, Holly interfaces with a range of POS systems, including those from Brink, Xenial, NCR, Aloha, and Oracle Microsystems. This focus on integration with existing infrastructure highlights Valyant AI’s approach of providing readily deployable \n",
      "NEWS 1 SOURCE: https://tinyurl.com/22nwfqzg\n",
      "\n",
      "KEYWORD: AI in Blockchain\n",
      "NEWS 2 TITLE: Worldcoin Extends Token Lock-Up, Triggering 42% Price Surge - Crypto News Australia\n",
      "NEWS 2 CONTENT: Worldcoin’s WLD token price jumped after announcing an extension of token lock-ups from 3 to 5 years.\n",
      "Over 6 million people have joined Worldcoin’s network by scanning their irises, securing a unique ID.\n",
      "The new unlock schedule suggests a slower release of WLD tokens, reaching 400 million by next September.\n",
      "Sam Altman’s Worldcoin has just released an update that has moved the market. WLD surged on news that token lock-ups have been extended. A blog post said that most of the WLD supply will be received by people with “verified World IDs – simply for being a unique human”.\n",
      "Related: Aussie Analyst Says BTC Reset Over, Prepare for What’s to Come\n",
      "The post stated that over 6 million people have already participated in the network by having their irises scanned.\n",
      "Further, they stated that Tools for Humanity (TFH), an early contributor to the Worldcoin project, informed the Worldcoin Foundation of an extension of the unlock schedule.\n",
      "Advertisement\n",
      "In a move aligned with the long-term nature of\n",
      "NEWS 2 SOURCE: https://tinyurl.com/27gozt8u\n",
      "\n",
      "KEYWORD: Transfer Learning\n",
      "NEWS 3 TITLE: Kylian Mbappe says playing for Real Madrid will complete childhood dream after official unveiling at Santiago Bernabeu - Deccan Herald\n",
      "NEWS 3 CONTENT: \"I wasn't the best at school but I had this dream of playing for Real Madrid and I knew that speaking the language would help me adapt,\" he said.\n",
      "Real Madrid announced in June that Mbappe was joining the La Liga side as a free agent on a five-year contract, ending a transfer saga that had rumbled on for several years.\n",
      "The club had courted Mbappe in the past but failed to land the player, who signed a contract extension with PSG in May 2022 when the Spanish club had a bid rejected.\n",
      "Mbappe was met on stage at the unveiling ceremony by compatriot and former Real Madrid player and manager Zinedine Zidane, who in 2012 had invited Mbappe to visit the club when he was 13-years-old, a visit that he described as an important moment in his life.\n",
      "The forward won six Ligue 1 titles, four French cups and became PSG's record goal scorer with 256 goals during seven years in Paris but failed to win the Champions League or the Ballon d'Or.\n",
      "Mbappe reached the semi-finals of the European Championship wit\n",
      "NEWS 3 SOURCE: https://tinyurl.com/23bzhl5p\n",
      "\n",
      "KEYWORD: AI Ethics\n",
      "NEWS 4 TITLE: Microsoft develops eerily realistic AI voice generator, but keeps it under wraps - Business Today\n",
      "NEWS 4 CONTENT: In a move that highlights the growing ethical concerns around advanced AI, Microsoft has developed a remarkably realistic text-to-speech system, VALL-E 2, but has chosen to keep it under wraps due to potential misuse.\n",
      "While often associated with flashy releases and wide availability, advancements in AI are increasingly forcing tech giants to tread carefully. Microsoft’s latest innovation, VALL-E 2, is a prime example of this trend. This AI marvel can mimic human speech with astonishing accuracy using just a few seconds of audio, marking a significant leap in text-to-speech (TTS) technology.\n",
      "“VALL-E 2 is the first voice AI to reach human parity in speech robustness, naturalness, and speaker simila\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/16/2024 11:39:27 PM - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mtweet_writer_agent\u001b[0m (to User):\n",
      "\n",
      "\u001b[32m***** Suggested tool call (call_tzaf): write_tweet_tool *****\u001b[0m\n",
      "Arguments: \n",
      "{\"source_list\":[\"\\u003chttps://tinyurl.com/22nwfqzg\\u003e\",\"\\u003chttps://tinyurl.com/27gozt8u\\u003e\",\"\\u003chttps://tinyurl.com/23bzhl5p\\u003e\",\"\\u003chttps://tinyurl.com/3cx8y82w\\u003e\"],\"tweet_list\":[\"ConverseNow acquires Valyant AI, boosting Voice AI in drive-thru restaurants. \\u003chttps://tinyurl.com/22nwfqzg\\u003e #AI #Restaurants\",\"Worldcoin's WLD token surges 42% after lock-up extension. Over 6M people have joined. \\u003chttps://tinyurl.com/27gozt8u\\u003e #AI #Blockchain\",\"Kylian Mbappe fulfills childhood dream joining Real Madrid, aiming to adapt with language skills. \\u003chttps://tinyurl.com/23bzhl5p\\u003e #AI #Ethics\",\"Microsoft develops highly realistic AI voice generator but keeps it secret due to ethical concerns. \\u003chttps://tinyurl.com/3cx8y82w\\u003e #AI #Ethics\"]}\n",
      "\u001b[32m*************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION write_tweet_tool...\u001b[0m\n",
      "\u001b[33mUser\u001b[0m (to tweet_writer_agent):\n",
      "\n",
      "\u001b[33mUser\u001b[0m (to tweet_writer_agent):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_tzaf) *****\u001b[0m\n",
      "\n",
      "Tweet: These are the AI news within the last 1 hour:\n",
      "11:00PM EST, July 16, 2024\n",
      "Length: 72\n",
      "                      \n",
      "                      \n",
      "Tweet: ConverseNow acquires Valyant AI, boosting Voice AI in drive-thru restaurants. <https://tinyurl.com/22nwfqzg> #AI #Restaurants\n",
      "Length: 125\n",
      "                      \n",
      "                      \n",
      "Tweet: Worldcoin's WLD token surges 42% after lock-up extension. Over 6M people have joined. <https://tinyurl.com/27gozt8u> #AI #Blockchain\n",
      "Length: 132\n",
      "                      \n",
      "                      \n",
      "Tweet: Kylian Mbappe fulfills childhood dream joining Real Madrid, aiming to adapt with language skills. <https://tinyurl.com/23bzhl5p> #AI #Ethics\n",
      "Length: 140\n",
      "                      \n",
      "                      \n",
      "Tweet: Microsoft develops highly realistic AI voice generator but keeps it secret due to ethical concerns. <https://tinyurl.com/3cx8y82w> #AI #Ethics\n",
      "Length: 142\n",
      "                      \n",
      "                      \n",
      "\u001b[32m**************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "try:\n",
    "    if AUTO_GENERATE_KEYWORDS=='True':\n",
    "        user_proxy_agent.initiate_chats([\n",
    "            {\n",
    "                \"recipient\": news_collector_agent,\n",
    "                \"message\": f\"Collect {KEYWORD_COUNT} news articles about the topic '{KEYWORD}' from the internet.\",\n",
    "                \"clear_history\": True,\n",
    "                \"silent\": False,\n",
    "                \"summary_method\": \"last_msg\"\n",
    "            },\n",
    "            {\n",
    "                \"recipient\": tweet_writer_agent,\n",
    "                \"message\": \"Write and post a twitter thread about the given list of news articles:\\n\",\n",
    "                \"clear_history\": True,\n",
    "                \"silent\": False,\n",
    "                \"summary_method\": \"last_msg\"\n",
    "            }\n",
    "        ])\n",
    "    else:\n",
    "        topics_list = pd.read_csv('../topics.csv')\n",
    "        topics_list = topics_list.values.tolist()\n",
    "        topics_list = [item for sublist in topics_list for item in sublist]\n",
    "        topics_list = [x for x in topics_list if str(x) != 'nan']\n",
    "        random.shuffle(topics_list)\n",
    "        topics_list = topics_list[:20] # TESTING\n",
    "\n",
    "        if len(topics_list)==0:\n",
    "            raise Exception(\"No topics found\")\n",
    "\n",
    "        news_articles = get_news_articles_tool(topics_list=topics_list, count=int(ARTICLE_COUNT))\n",
    "        if news_articles:\n",
    "            user_proxy_agent.initiate_chats([\n",
    "                {\n",
    "                    \"recipient\": tweet_writer_agent,\n",
    "                    \"message\": f\"Write and post a twitter thread about the given list of news articles:\\n{news_articles}\",\n",
    "                    \"clear_history\": True,\n",
    "                    \"silent\": False,\n",
    "                    \"summary_method\": \"last_msg\"\n",
    "                }\n",
    "            ])\n",
    "        else:\n",
    "            raise Exception(\"No news articles found\")\n",
    "except Exception as e:\n",
    "    print(f\"Global Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tweetbot-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

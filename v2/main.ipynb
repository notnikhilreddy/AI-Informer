{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from autogen import UserProxyAgent, AssistantAgent\n",
    "from gnews import GNews\n",
    "from twikit import Client\n",
    "import os\n",
    "\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "KEYWORD = os.getenv(\"KEYWORD\")\n",
    "ARTICLE_COUNT = os.getenv(\"ARTICLE_COUNT\")\n",
    "KEYWORD_COUNT = os.getenv(\"KEYWORD_COUNT\")\n",
    "NEWS_COUNTRY = os.getenv(\"NEWS_COUNTRY\")\n",
    "GROQ_MODEL_NAME = os.getenv(\"GROQ_MODEL_NAME\")\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "GROQ_API_BASE = os.getenv(\"GROQ_API_BASE\")\n",
    "RELEASE = os.getenv(\"RELEASE\")\n",
    "AUTO_GENERATE_KEYWORDS = os.getenv(\"AUTO_GENERATE_KEYWORDS\")\n",
    "VERSION = os.getenv(\"VERSION\")\n",
    "\n",
    "if(RELEASE == \"PROD\"):\n",
    "    USERNAME = os.getenv(\"XUSERNAME\")\n",
    "    EMAIL = os.getenv(\"XEMAIL\")\n",
    "    PASSWORD = os.getenv(\"XPASSWORD\")\n",
    "else:\n",
    "    USERNAME = os.getenv(\"XUSERNAME_TEST\")\n",
    "    EMAIL = os.getenv(\"XEMAIL_TEST\")\n",
    "    PASSWORD = os.getenv(\"XPASSWORD_TEST\")\n",
    "\n",
    "#print all the above environment variables\n",
    "# for key in ['KEYWORD', 'ARTICLE_COUNT', 'KEYWORD_COUNT', 'NEWS_COUNTRY', 'AUTO_GENERATE_KEYWORDS',\n",
    "#             'GROQ_MODEL_NAME', 'GROQ_API_BASE', 'RELEASE', 'VERSION',]:\n",
    "#     print(f\"{key} = {os.environ[key]}\")\n",
    "\n",
    "# create cache directory\n",
    "cache = '../.cache'\n",
    "if not os.path.exists(cache):\n",
    "    os.makedirs(cache)\n",
    "topics_file = f'{cache}/topics.csv'\n",
    "urls_file = f'{cache}/urls.csv'\n",
    "\n",
    "# Config dictionary\n",
    "llm_config = {\n",
    "    \"cache_seed\": 42,\n",
    "    \"config_list\": [{\n",
    "        \"model\": GROQ_MODEL_NAME,\n",
    "        \"api_key\": GROQ_API_KEY,\n",
    "        \"base_url\": GROQ_API_BASE\n",
    "    }],\n",
    "}\n",
    "\n",
    "google_news = GNews()\n",
    "google_news.period = '1h'  # News from last 7 days\n",
    "google_news.max_results = int(ARTICLE_COUNT)  # number of responses across a keyword\n",
    "google_news.country = 'United States'  # News from a specific country \n",
    "google_news.language = 'english'  # News in a specific language\n",
    "\n",
    "# # Initialize client\n",
    "if RELEASE != 'DEV' and 'x_client' not in globals():\n",
    "    x_client = Client('en-US')\n",
    "\n",
    "    x_client.login(\n",
    "        auth_info_1=USERNAME,\n",
    "        auth_info_2=EMAIL,\n",
    "        password=PASSWORD\n",
    "    )\n",
    "    print(\"Client initialized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Decode encoded Google News entry URLs.\"\"\"\n",
    "import base64\n",
    "import functools\n",
    "import re\n",
    "\n",
    "\n",
    "_ENCODED_URL_PREFIX = \"https://news.google.com/rss/articles/\"\n",
    "_ENCODED_URL_RE = re.compile(fr\"^{re.escape(_ENCODED_URL_PREFIX)}(?P<encoded_url>[^?]+)\")\n",
    "_DECODED_URL_RE = re.compile(rb'^\\x08\\x13\".+?(?P<primary_url>http[^\\xd2]+)\\xd2\\x01')\n",
    "\n",
    "@functools.lru_cache(2048)\n",
    "def _decode_google_news_url(url: str) -> str:\n",
    "    match = _ENCODED_URL_RE.match(url)\n",
    "    encoded_text = match.groupdict()[\"encoded_url\"]  # type: ignore\n",
    "    encoded_text += \"===\"  # Fix incorrect padding. Ref: https://stackoverflow.com/a/49459036/\n",
    "    decoded_text = base64.urlsafe_b64decode(encoded_text)\n",
    "\n",
    "    match = _DECODED_URL_RE.match(decoded_text)\n",
    "    primary_url = match.groupdict()[\"primary_url\"]  # type: ignore\n",
    "    primary_url = primary_url.decode()\n",
    "    return primary_url\n",
    "\n",
    "\n",
    "def decode_google_news_url(url: str) -> str:  # Not cached because not all Google News URLs are encoded.\n",
    "    \"\"\"Return Google News entry URLs after decoding their encoding as applicable.\"\"\"\n",
    "    return _decode_google_news_url(url) if url.startswith(_ENCODED_URL_PREFIX) else url\n",
    "\n",
    "\n",
    "def deduplicate_news_list(urls, keywords):\n",
    "    # deduplicate the urls and keywords based on urls\n",
    "    urls_dict = {}\n",
    "    for i in range(len(urls)):\n",
    "        if urls[i] not in urls_dict:\n",
    "            urls_dict[urls[i]] = keywords[i]\n",
    "        else:\n",
    "            urls_dict[urls[i]] = urls_dict[urls[i]] + ', ' + keywords[i]\n",
    "\n",
    "    urls = list(urls_dict.keys())\n",
    "    keywords = list(urls_dict.values())\n",
    "    return urls, keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyshorteners import Shortener\n",
    "from newspaper import Article\n",
    "from typing import Annotated\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def read_news_articles_tool(urls, keywords):\n",
    "    if not os.path.isfile(urls_file):\n",
    "        df_urls = pd.DataFrame(columns=['urls', 'status'])  # Define the variable with a default value\n",
    "        df_urls.to_csv(urls_file)\n",
    "    else:\n",
    "        df_urls = pd.read_csv(urls_file, index_col='Unnamed: 0')\n",
    "    \n",
    "    urls = [url for url in urls if url not in df_urls['urls'].values]\n",
    "    \n",
    "    #deduplicate the urls and keywords based on urls\n",
    "    urls, keywords = deduplicate_news_list(urls, keywords)\n",
    "\n",
    "    if len(urls) == 0:\n",
    "        return []\n",
    "\n",
    "    article_list = []\n",
    "    for i in range(len(urls)):\n",
    "        try:\n",
    "            \n",
    "            article = Article(urls[i])\n",
    "            article.download()\n",
    "            article.parse()\n",
    "\n",
    "            if article.text: # and len(article.text.strip().split('\\n')) > 1:\n",
    "                # Append the URL and status to the DataFrame\n",
    "                df_urls = pd.concat([pd.DataFrame([[urls[i], 'success']], columns=df_urls.columns), df_urls], ignore_index=True)\n",
    "                df_urls.to_csv(urls_file)\n",
    "                article.keyword = keywords[i]\n",
    "                article_list.append(article)\n",
    "                continue\n",
    "            else:\n",
    "                df_urls = pd.concat([pd.DataFrame([[urls[i], 'empty']], columns=df_urls.columns), df_urls], ignore_index=True)\n",
    "                df_urls.to_csv(urls_file)\n",
    "                continue\n",
    "        except Exception as e:\n",
    "            df_urls = pd.concat([pd.DataFrame([[urls[i], 'error']], columns=df_urls.columns), df_urls], ignore_index=True)\n",
    "            df_urls.to_csv(urls_file)\n",
    "            print(f\"Error reading article: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    return article_list\n",
    "\n",
    "\n",
    "def get_news_articles_tool(keyword_list: Annotated[list, \"The list of keywords\"], count: Annotated[int, \"The number of news articles to collect from the internet\"]) -> str:\n",
    "    s = Shortener(timeout=5)\n",
    "    \n",
    "    urls, keywords = [], []\n",
    "    for keyword in keyword_list:\n",
    "        # keyword = keyword.replace(\" \", \"%20\")\n",
    "        print(f\"FETCHING NEWS ON TOPIC: {keyword}\")\n",
    "        sources = google_news.get_news(keyword)\n",
    "        # source.build(keyword = keyword, topic = 'TECHNOLOGY', top_news=False)\n",
    "        for source in sources:\n",
    "            decoded_url = decode_google_news_url(source['url'])\n",
    "            urls = urls + [decoded_url]\n",
    "        keywords = keywords + [keyword] * len(sources)\n",
    "    print(f\"URLS: {urls}\")\n",
    "\n",
    "    if len(urls) == 0:\n",
    "        return None\n",
    "    \n",
    "    article_list = read_news_articles_tool(urls, keywords)\n",
    "    print(f\"Articles read: {len(article_list)}\")\n",
    "\n",
    "    result = ''\n",
    "    if len(article_list) > 0:\n",
    "        # Get the short urls\n",
    "        for article in article_list:\n",
    "            try:\n",
    "                article.short_url = s.tinyurl.short(article.url)\n",
    "            except Exception as e:\n",
    "                print(f\"Error shortening url: {str(e)}\")\n",
    "                article.short_url = article.url\n",
    "\n",
    "        for i in range(len(article_list)):\n",
    "            # do this until the result length is less than 30000\n",
    "            if len(result) > 30000:\n",
    "                break\n",
    "            result += (\"\"\"NEWS {n} TOPIC: {keyword}\n",
    "NEWS {n} TITLE: {title}\n",
    "NEWS {n} CONTENT: {content}\n",
    "NEWS {n} SOURCE: {url}\n",
    "\n",
    "\"\"\"\n",
    "        ).format(\n",
    "            keyword = article_list[i].keyword,\n",
    "            n = i+1,\n",
    "            title=article_list[i].title,\n",
    "            content=article_list[i].text.replace('\\n\\n', '\\n')[:1000],\n",
    "            url=article_list[i].short_url\n",
    "        )\n",
    "\n",
    "    return result[:4500]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "\n",
    "def merge_tweets(tweet_list: Annotated[list, \"The list of tweets to merge\"]) -> None:\n",
    "    merged_tweets = []\n",
    "    current_tweet = \"\"\n",
    "\n",
    "    for tweet in tweet_list:\n",
    "        if len(current_tweet) + len(tweet) <= 278:\n",
    "            current_tweet += f'{tweet}\\n\\n'\n",
    "        else:\n",
    "            merged_tweets.append(current_tweet)\n",
    "            current_tweet = tweet\n",
    "    if current_tweet:\n",
    "        merged_tweets.append(current_tweet)\n",
    "    \n",
    "    return merged_tweets\n",
    "\n",
    "def add_source_urls(tweet_list: Annotated[list, \"The list of tweets to post\"], source_list: Annotated[list, \"The list of 'https://tinyurl.com/' source URLs for each tweet\"]) -> list:\n",
    "    if len(tweet_list) == len(source_list):\n",
    "        del_index = []\n",
    "        for i in range(len(source_list)):\n",
    "            if tweet_list[i] and source_list[i]:\n",
    "                if not re.search(r'https://tinyurl\\.com/[a-zA-Z0-9]{8}', tweet_list[i]): # if the tweet does not contain a source\n",
    "                    if re.search(r'https://tinyurl\\.com/[a-zA-Z0-9]{8}', source_list[i]): # if we have a source seperately\n",
    "                        tweet_list[i] = tweet_list[i][:278-len(source_list[i])]\n",
    "                        tweet_list[i] += f\"\\n{source_list[i]}\"\n",
    "        # uncomment below five lines to don't post a news if it doesn't have a source\n",
    "                    else:\n",
    "                        del_index.append(i)\n",
    "        for i in sorted(del_index, reverse=True):\n",
    "            del tweet_list[i]\n",
    "            del source_list[i]\n",
    "        \n",
    "    return tweet_list\n",
    "\n",
    "def get_intro_tweet() -> str:\n",
    "    now = datetime.now(pytz.utc)\n",
    "    eastern = pytz.timezone('America/New_York')\n",
    "    now_eastern = now.astimezone(eastern)\n",
    "\n",
    "    last_hour = now_eastern.replace(minute=0, second=0, microsecond=0)\n",
    "\n",
    "    formatted_datetime = last_hour.strftime(\"%I:00%p EST, %B %d, %Y\")\n",
    "\n",
    "    return f\"\"\"These are the AI news within the last 1 hour:\n",
    "{formatted_datetime}\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def write_tweet_tool(tweet_list: Annotated[list, \"The list of tweets to post\"], source_list: Annotated[list, \"The list of 'https://tinyurl.com/' source URL for each tweet\"]) -> str:\n",
    "    tweet_list = add_source_urls(tweet_list, source_list)\n",
    "    # tweet_list = merge_tweets(tweet_list)\n",
    "    # tweet_list = [get_intro_tweet()] + tweet_list\n",
    "\n",
    "    posts = ''\n",
    "    # for tweet, source in zip(tweet_list, source_list):\n",
    "    for i in range(len(tweet_list)):\n",
    "    #     if 'https://tinyurl.com/' in tweet_list[i]:\n",
    "    #         if len(tweet_list[i]) > 280:\n",
    "    #             tweet_list[i] = tweet_list[i][:276-len(source)] + '...' + f\"\\n{source}\"\n",
    "    #     else:\n",
    "    #         if len(tweet_list[i]) <= 279-len(source):\n",
    "    #             tweet_list[i] += f\"\\n{source}\"\n",
    "    #         else:\n",
    "    #             tweet_list[i] = tweet_list[i][:276-len(source)] + '...' + f\"\\n{source}\"\n",
    "        try:\n",
    "            # final tweet length check for redundancy \n",
    "            if len(tweet_list[i]) > 280:\n",
    "                tweet_list[i] = tweet_list[i][:276] + '...'\n",
    "\n",
    "            if RELEASE != \"DEV\":\n",
    "                if i==0:\n",
    "                    last_tweet = x_client.create_tweet(\n",
    "                        text=tweet_list[i],\n",
    "                    )\n",
    "                else:\n",
    "                    last_tweet = x_client.create_tweet(\n",
    "                        text=tweet_list[i],\n",
    "                        reply_to=last_tweet.id\n",
    "                    )\n",
    "            \n",
    "            posts += (\"\"\"\n",
    "Tweet: {tweet}\n",
    "Length: {length}\n",
    "                      \n",
    "                      \"\"\").format(tweet=tweet_list[i], length=len(tweet_list[i]))\n",
    "            \n",
    "            time.sleep(1)\n",
    "        except Exception as e:\n",
    "            error_message = f\"Failed to post tweet: {str(e)}\"\n",
    "            print(error_message)\n",
    "            continue\n",
    "\n",
    "    if(posts == ''):\n",
    "        posts = \"No tweets posted\"\n",
    "        \n",
    "    return posts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tweet: Online learning issues reflect pre-existing weaknesses in education. Remote education simply exposes the problems. #DeepLearning #OnlineEducation\n",
      "Length: 145\n",
      "                      \n",
      "                      \n"
     ]
    }
   ],
   "source": [
    "# test the function\n",
    "print(write_tweet_tool(**{\"tweet_list\":[\"States are taking the lead in AI regulation as Congress stalls. Colorado's new law sets guardrails for AI development and use, focusing on consumer harm and discrimination. #ArtificialIntelligence #AIRegulation\",\"Meta suspends AI tools in Brazil amid privacy policy dispute. Brazil's data protection authority objects to generative AI personal data processing. #ArtificialIntelligence #MetaAI\",\"Equifax sees cost savings and innovation with cloud and AI adoption. 89% of new models and scores now built using AI and Machine Learning. #ArtificialIntelligence #MachineLearning\",\"Online learning issues reflect pre-existing weaknesses in education. Remote education simply exposes the problems. #DeepLearning #OnlineEducation\"],\"source_list\":[\"https://tinyurl.com/***9jgd33\",\"https://tinyurl.com/***cangxta\",\"https://tinyurl.com/***cjxu6mb\",None]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_collector_agent = AssistantAgent(\n",
    "    \"news_collector_agent\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=f\"\"\"You are good at collecting recent news articles about a given keyword on the internet. \n",
    "    You should generate a list of {KEYWORD_COUNT} topics closely related to the given keyword. \n",
    "    Use the provided tool to collect news about the generated list of topics.\"\"\",\n",
    "    max_consecutive_auto_reply=1\n",
    ")\n",
    "\n",
    "tweet_writer_agent = AssistantAgent(\n",
    "    \"tweet_writer_agent\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=f\"\"\"You are an autonomous twitter bot that's created to educate the people about {KEYWORD}. \n",
    "    You are good at posting a series of twitter posts on the given list of news by summarizing each news as one short tweet. \n",
    "    You MUST only strictly post news that is about the topic {KEYWORD} or the respective news topic given and ignore other news(double check this). \n",
    "    Always use simple words. \n",
    "    Use the provided tool to post all the tweets as a thread(list of tweets).\"\"\",\n",
    "    max_consecutive_auto_reply=1\n",
    ")\n",
    "\n",
    "\n",
    "user_proxy_agent = UserProxyAgent(\n",
    "    name=\"User\",\n",
    "    system_message=\"You are a helpful AI assistant. Return 'TERMINATE' when the task is done.\",\n",
    "    is_termination_msg=lambda msg: msg.get(\"content\") is not None and \"TERMINATE\" in msg[\"content\"],\n",
    "    human_input_mode=\"NEVER\",\n",
    "    code_execution_config=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.write_tweet_tool(tweet_list: Annotated[list, 'The list of tweets to post'], source_list: Annotated[list, \"The list of 'https://tinyurl.com/' source URL for each tweet\"]) -> str>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Register the tool signature with the assistant agent.\n",
    "news_collector_agent.register_for_llm(name=\"get_news_articles_tool\", description=\"Collect news articles about a list of topics on the internet.\")(get_news_articles_tool)\n",
    "tweet_writer_agent.register_for_llm(name=\"write_tweet_tool\", description=\"Write a twitter thread.\")(write_tweet_tool)\n",
    "\n",
    "# Register the tool function with the user proxy agent.\n",
    "user_proxy_agent.register_for_execution(name=\"get_news_articles_tool\")(get_news_articles_tool)\n",
    "user_proxy_agent.register_for_execution(name=\"write_tweet_tool\")(write_tweet_tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[34mStarting a new chat....\u001b[0m\n",
      "\u001b[34m\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[33mUser\u001b[0m (to news_collector_agent):\n",
      "\n",
      "Collect 5 news articles about the topic 'Artificial Intelligence' from the internet.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mnews_collector_agent\u001b[0m (to User):\n",
      "\n",
      "\u001b[32m***** Suggested tool call (call_d4c6): get_news_articles_tool *****\u001b[0m\n",
      "Arguments: \n",
      "{\"count\":5,\"keyword_list\":[\"Artificial Intelligence\",\"AI technology\",\"Machine Learning\",\"Neural Networks\",\"Deep Learning\"]}\n",
      "\u001b[32m*******************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION get_news_articles_tool...\u001b[0m\n",
      "FETCHING NEWS ON TOPIC: Artificial Intelligence\n",
      "Decoded URL: https://news.northeastern.edu/2024/07/18/healey-ai-task-force-northeastern/\n",
      "Decoded URL: https://www.washingtonpost.com/technology/2024/07/18/solarwinds-sec-cybersecurity-hack-disclosures/\n",
      "Decoded URL: https://www.nortonrosefulbright.com/en-id/knowledge/publications/7052eff6/artificial-intelligence-and-liability\n",
      "FETCHING NEWS ON TOPIC: AI technology\n",
      "Decoded URL: https://news.northeastern.edu/2024/07/18/healey-ai-task-force-northeastern/\n",
      "Decoded URL: https://www.channelfutures.com/artificial-intelligence/survey-contact-center-agents-want-generative-ai-tools\n",
      "Decoded URL: https://techcrunch.com/2024/07/18/chatgpt-everything-to-know-about-the-ai-chatbot/\n",
      "FETCHING NEWS ON TOPIC: Machine Learning\n",
      "Decoded URL: https://www.forbes.com/sites/lanceeliot/2024/07/18/why-americans-believe-that-generative-ai-such-as-chatgpt-has-consciousness/\n",
      "Decoded URL: https://www.financialexpress.com/jobs-career/new-age-skills-ai-machine-learning-jobs-highest-paid-shows-report-3557747/\n",
      "Decoded URL: https://www.cbr.com/dark-horse-comics-position-on-ai-art/\n",
      "FETCHING NEWS ON TOPIC: Neural Networks\n",
      "Decoded URL: https://www.nature.com/articles/s41598-024-66392-4\n",
      "FETCHING NEWS ON TOPIC: Deep Learning\n",
      "URLS: ['https://news.northeastern.edu/2024/07/18/healey-ai-task-force-northeastern/', 'https://www.washingtonpost.com/technology/2024/07/18/solarwinds-sec-cybersecurity-hack-disclosures/', 'https://www.nortonrosefulbright.com/en-id/knowledge/publications/7052eff6/artificial-intelligence-and-liability', 'https://news.northeastern.edu/2024/07/18/healey-ai-task-force-northeastern/', 'https://www.channelfutures.com/artificial-intelligence/survey-contact-center-agents-want-generative-ai-tools', 'https://techcrunch.com/2024/07/18/chatgpt-everything-to-know-about-the-ai-chatbot/', 'https://www.forbes.com/sites/lanceeliot/2024/07/18/why-americans-believe-that-generative-ai-such-as-chatgpt-has-consciousness/', 'https://www.financialexpress.com/jobs-career/new-age-skills-ai-machine-learning-jobs-highest-paid-shows-report-3557747/', 'https://www.cbr.com/dark-horse-comics-position-on-ai-art/', 'https://www.nature.com/articles/s41598-024-66392-4']\n",
      "Articles read: 2\n",
      "\u001b[33mUser\u001b[0m (to news_collector_agent):\n",
      "\n",
      "\u001b[33mUser\u001b[0m (to news_collector_agent):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_d4c6) *****\u001b[0m\n",
      "NEWS 1 TOPIC: Artificial Intelligence\n",
      "NEWS 1 TITLE: ChatGPT: Everything you need to know about the AI chatbot\n",
      "NEWS 1 CONTENT: ChatGPT, OpenAI’s text-generating AI chatbot, has taken the world by storm since its launch in November 2022. What started as a tool to hyper-charge productivity through writing essays and code with short text prompts has evolved into a behemoth used by more than 92% of Fortune 500 companies.\n",
      "That growth has propelled OpenAI itself into becoming one of the most-hyped companies in recent memory. And its latest partnership with Apple for its upcoming generative AI offering, Apple Intelligence, has given the company another significant bump in the AI race.\n",
      "2024 also saw the release of GPT-4o, OpenAI’s new flagship omni model for ChatGPT. GPT-4o is now the default free model, complete with voice and vision capabilities. But after demoing GPT-4o, OpenAI paused one of its voices, Sky, after allegations that it was mimicking Scarlett Johansson’s voice in “Her.”\n",
      "OpenAI is facing internal drama, including the sizable exit of co-founder and longtime chief scientist Ilya Sutskever as the company \n",
      "NEWS 1 SOURCE: https://tinyurl.com/234jmnys\n",
      "\n",
      "NEWS 2 TOPIC: Artificial Intelligence\n",
      "NEWS 2 TITLE: New-age skills: AI, machine learning jobs highest paid, shows report\n",
      "NEWS 2 CONTENT: Artificial intelligence (AI) and machine learning (ML) jobs are the highest paid among the new-age roles offered by the corporate sector, according to a report.\n",
      "The report by talent platform foundit released on Thursday showed that the freshers in AI/ML roles have the highest average minimum salary (Rs 7.8 lakh per annum) and average maximum salary (Rs 10.3 lakh per annum), indicating a strong demand for the skills even at the entry level.\n",
      "The AI/ML professionals are also the highest paid in the mid- and senior-levels across the new-age roles. Mid-level AI/ML professionals (7-10 years of work experience) have an average maximum salary of Rs 28.6 lakh per annum, followed by Rs 25.3 lakh for software development jobs.\n",
      "Also Read ‘Skilling is now a board-level conversation in India’ 25,000 job seekers end up in stampede-like situation at Air India Airport Services’ recruitment drive in Mumbai CDIOs replacing CIOs amid high talent demand Karnataka Gig Workers Bill: Why aggregators are upset\n",
      "NEWS 2 SOURCE: https://tinyurl.com/2da52am6\n",
      "\n",
      "\n",
      "\u001b[32m**************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[34m\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[34mStarting a new chat....\u001b[0m\n",
      "\u001b[34m\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[33mUser\u001b[0m (to tweet_writer_agent):\n",
      "\n",
      "Write and post a twitter thread about the given list of news articles:\n",
      "\n",
      "Context: \n",
      "NEWS 1 TOPIC: Artificial Intelligence\n",
      "NEWS 1 TITLE: ChatGPT: Everything you need to know about the AI chatbot\n",
      "NEWS 1 CONTENT: ChatGPT, OpenAI’s text-generating AI chatbot, has taken the world by storm since its launch in November 2022. What started as a tool to hyper-charge productivity through writing essays and code with short text prompts has evolved into a behemoth used by more than 92% of Fortune 500 companies.\n",
      "That growth has propelled OpenAI itself into becoming one of the most-hyped companies in recent memory. And its latest partnership with Apple for its upcoming generative AI offering, Apple Intelligence, has given the company another significant bump in the AI race.\n",
      "2024 also saw the release of GPT-4o, OpenAI’s new flagship omni model for ChatGPT. GPT-4o is now the default free model, complete with voice and vision capabilities. But after demoing GPT-4o, OpenAI paused one of its voices, Sky, after allegations that it was mimicking Scarlett Johansson’s voice in “Her.”\n",
      "OpenAI is facing internal drama, including the sizable exit of co-founder and longtime chief scientist Ilya Sutskever as the company \n",
      "NEWS 1 SOURCE: https://tinyurl.com/234jmnys\n",
      "\n",
      "NEWS 2 TOPIC: Artificial Intelligence\n",
      "NEWS 2 TITLE: New-age skills: AI, machine learning jobs highest paid, shows report\n",
      "NEWS 2 CONTENT: Artificial intelligence (AI) and machine learning (ML) jobs are the highest paid among the new-age roles offered by the corporate sector, according to a report.\n",
      "The report by talent platform foundit released on Thursday showed that the freshers in AI/ML roles have the highest average minimum salary (Rs 7.8 lakh per annum) and average maximum salary (Rs 10.3 lakh per annum), indicating a strong demand for the skills even at the entry level.\n",
      "The AI/ML professionals are also the highest paid in the mid- and senior-levels across the new-age roles. Mid-level AI/ML professionals (7-10 years of work experience) have an average maximum salary of Rs 28.6 lakh per annum, followed by Rs 25.3 lakh for software development jobs.\n",
      "Also Read ‘Skilling is now a board-level conversation in India’ 25,000 job seekers end up in stampede-like situation at Air India Airport Services’ recruitment drive in Mumbai CDIOs replacing CIOs amid high talent demand Karnataka Gig Workers Bill: Why aggregators are upset\n",
      "NEWS 2 SOURCE: https://tinyurl.com/2da52am6\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/18/2024 07:56:53 PM - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mtweet_writer_agent\u001b[0m (to User):\n",
      "\n",
      "\u001b[32m***** Suggested tool call (call_1kqy): write_tweet_tool *****\u001b[0m\n",
      "Arguments: \n",
      "{\"source_list\":[\"\\u003chttps://tinyurl.com/234jmnys\\u003e\",\"\\u003chttps://tinyurl.com/2da52am6\\u003e\"],\"tweet_list\":[\"ChatGPT, OpenAI’s text-generating AI chatbot, is used by 92% of Fortune 500 companies. Its latest partnership with Apple for Apple Intelligence has propelled OpenAI into the AI race.\",\"AI \\u0026 machine learning jobs have the highest average minimum \\u0026 maximum salary among new-age roles. Freshers in AI/ML roles earn an average of 7.8-10.3 lakh per annum, according to a report.\"]}\n",
      "\u001b[32m*************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION write_tweet_tool...\u001b[0m\n",
      "\u001b[33mUser\u001b[0m (to tweet_writer_agent):\n",
      "\n",
      "\u001b[33mUser\u001b[0m (to tweet_writer_agent):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_1kqy) *****\u001b[0m\n",
      "\n",
      "Tweet: ChatGPT, OpenAI’s text-generating AI chatbot, is used by 92% of Fortune 500 companies. Its latest partnership with Apple for Apple Intelligence has propelled OpenAI into the AI race.\n",
      "<https://tinyurl.com/234jmnys>\n",
      "Length: 213\n",
      "                      \n",
      "                      \n",
      "Tweet: AI & machine learning jobs have the highest average minimum & maximum salary among new-age roles. Freshers in AI/ML roles earn an average of 7.8-10.3 lakh per annum, according to a report.\n",
      "<https://tinyurl.com/2da52am6>\n",
      "Length: 219\n",
      "                      \n",
      "                      \n",
      "\u001b[32m**************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "try:\n",
    "    if AUTO_GENERATE_KEYWORDS=='True':\n",
    "        user_proxy_agent.initiate_chats([\n",
    "            {\n",
    "                \"recipient\": news_collector_agent,\n",
    "                \"message\": f\"Collect {KEYWORD_COUNT} news articles about the topic '{KEYWORD}' from the internet.\",\n",
    "                \"clear_history\": True,\n",
    "                \"silent\": False,\n",
    "                \"summary_method\": \"last_msg\"\n",
    "            },\n",
    "            {\n",
    "                \"recipient\": tweet_writer_agent,\n",
    "                \"message\": \"Write and post a twitter thread about the given list of news articles:\\n\",\n",
    "                \"clear_history\": True,\n",
    "                \"silent\": False,\n",
    "                \"summary_method\": \"last_msg\"\n",
    "            }\n",
    "        ])\n",
    "    else:\n",
    "        topics_list = pd.read_csv('../topics.csv')\n",
    "        topics_list = topics_list.values.tolist()\n",
    "        topics_list = [item for sublist in topics_list for item in sublist]\n",
    "        topics_list = [x for x in topics_list if str(x) != 'nan']\n",
    "        random.shuffle(topics_list)\n",
    "        # topics_list = topics_list[:5] # TESTING\n",
    "\n",
    "        if len(topics_list)==0:\n",
    "            raise Exception(\"No topics found\")\n",
    "\n",
    "        news_articles = get_news_articles_tool(keyword_list=topics_list, count=int(ARTICLE_COUNT))\n",
    "        if news_articles:\n",
    "            user_proxy_agent.initiate_chats([\n",
    "                {\n",
    "                    \"recipient\": tweet_writer_agent,\n",
    "                    \"message\": f\"Write and post a twitter thread about the given list of news articles:\\n{news_articles}\",\n",
    "                    \"clear_history\": True,\n",
    "                    \"silent\": False,\n",
    "                    \"summary_method\": \"last_msg\"\n",
    "                }\n",
    "            ])\n",
    "        else:\n",
    "            raise Exception(\"No news articles found\")\n",
    "except Exception as e:\n",
    "    print(f\"Global Error: {str(e)}\")\n",
    "    #print trackback\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tweetbot-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
